#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„PEFTæ¨¡å‹åˆå¹¶è„šæœ¬

è¿™ä¸ªè„šæœ¬å¸®åŠ©æ‚¨:
1. å°†è®­ç»ƒå¥½çš„LoRA/QLoRAé€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶
2. ä¿å­˜ä¸ºå®Œæ•´çš„æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ç”¨äºæ¨ç†
3. éªŒè¯åˆå¹¶åçš„æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ

ä½¿ç”¨ç¤ºä¾‹:
    # åˆå¹¶LoRAé€‚é…å™¨
    python merge_peft_model.py --base_model Qwen/Qwen2.5-7B-Instruct --adapter_path ./outputs/sft_lora/checkpoint-1000 --output_path ./merged_model
    
    # éªŒè¯åˆå¹¶åçš„æ¨¡å‹
    python merge_peft_model.py --validate ./merged_model
"""

import os
import sys
import argparse
import torch
import json
from pathlib import Path
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer
)

try:
    from peft import PeftModel
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False
    print("âŒ PEFTåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install peft")
    sys.exit(1)


def merge_peft_model(base_model_path: str, 
                    adapter_path: str, 
                    output_path: str,
                    device: str = "auto",
                    torch_dtype: str = "float16"):
    """
    åˆå¹¶PEFTé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹å
        adapter_path: LoRAé€‚é…å™¨è·¯å¾„
        output_path: åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„
        device: è®¾å¤‡ (cpu, cuda, auto)
        torch_dtype: æ•°æ®ç±»å‹ (float16, bfloat16, float32)
    """
    print("ğŸš€ å¼€å§‹åˆå¹¶PEFTæ¨¡å‹...")
    print(f"ğŸ“ åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"ğŸ”§ é€‚é…å™¨è·¯å¾„: {adapter_path}")
    print(f"ğŸ’¾ è¾“å‡ºè·¯å¾„: {output_path}")
    
    # è®¾ç½®æ•°æ®ç±»å‹
    dtype_map = {
        "float16": torch.float16,
        "bfloat16": torch.bfloat16,
        "float32": torch.float32
    }
    torch_dtype = dtype_map.get(torch_dtype, torch.float16)
    
    try:
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        print("\nğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
        model_kwargs = {
            "torch_dtype": torch_dtype,
            "trust_remote_code": True
        }
        
        if device != "cpu":
            model_kwargs["device_map"] = "auto" if device == "auto" else device
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            **model_kwargs
        )
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 2. åŠ è½½åˆ†è¯å™¨
        print("ğŸ“ åŠ è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # 3. åŠ è½½PEFTé€‚é…å™¨
        print("ğŸ”§ åŠ è½½PEFTé€‚é…å™¨...")
        model = PeftModel.from_pretrained(base_model, adapter_path)
        print("âœ… PEFTé€‚é…å™¨åŠ è½½æˆåŠŸ")
        
        # 4. åˆå¹¶é€‚é…å™¨
        print("ğŸ”„ åˆå¹¶é€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹...")
        merged_model = model.merge_and_unload()
        print("âœ… æ¨¡å‹åˆå¹¶æˆåŠŸ")
        
        # 5. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        print(f"ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
        os.makedirs(output_path, exist_ok=True)
        
        merged_model.save_pretrained(
            output_path,
            safe_serialization=True,
            max_shard_size="5GB"
        )
        
        tokenizer.save_pretrained(output_path)
        
        # 6. ä¿å­˜åˆå¹¶ä¿¡æ¯
        merge_info = {
            "base_model_path": base_model_path,
            "adapter_path": adapter_path,
            "output_path": output_path,
            "torch_dtype": str(torch_dtype),
            "device": device,
            "status": "success"
        }
        
        with open(os.path.join(output_path, "merge_info.json"), 'w', encoding='utf-8') as f:
            json.dump(merge_info, f, indent=2, ensure_ascii=False)
        
        print("\nğŸ‰ æ¨¡å‹åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“ åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨: {output_path}")
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f"   from transformers import AutoModelForCausalLM, AutoTokenizer")
        print(f"   model = AutoModelForCausalLM.from_pretrained('{output_path}')")
        print(f"   tokenizer = AutoTokenizer.from_pretrained('{output_path}')")
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        raise


def validate_model(model_path: str, sample_text: str = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"):
    """
    éªŒè¯æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸å·¥ä½œ
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        sample_text: æµ‹è¯•æ–‡æœ¬
    """
    print(f"ğŸ” éªŒè¯æ¨¡å‹: {model_path}")
    
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆ
        print(f"ğŸ§ª æµ‹è¯•ç”Ÿæˆï¼Œè¾“å…¥: {sample_text}")
        inputs = tokenizer(sample_text, return_tensors="pt")
        
        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_length=inputs.input_ids.shape[1] + 50,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ“ ç”Ÿæˆç»“æœ: {generated_text}")
        print("âœ… æ¨¡å‹éªŒè¯æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
        return False


def check_adapter_info(adapter_path: str):
    """
    æ£€æŸ¥é€‚é…å™¨ä¿¡æ¯
    
    Args:
        adapter_path: é€‚é…å™¨è·¯å¾„
    """
    print(f"ğŸ” æ£€æŸ¥é€‚é…å™¨ä¿¡æ¯: {adapter_path}")
    
    # æ£€æŸ¥é€‚é…å™¨é…ç½®
    adapter_config_path = os.path.join(adapter_path, "adapter_config.json")
    if os.path.exists(adapter_config_path):
        with open(adapter_config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        print("ğŸ“‹ é€‚é…å™¨é…ç½®:")
        print(f"   ç±»å‹: {config.get('peft_type', 'Unknown')}")
        print(f"   ä»»åŠ¡ç±»å‹: {config.get('task_type', 'Unknown')}")
        print(f"   åŸºç¡€æ¨¡å‹: {config.get('base_model_name_or_path', 'Unknown')}")
        
        if 'r' in config:
            print(f"   LoRA rank: {config['r']}")
        if 'lora_alpha' in config:
            print(f"   LoRA alpha: {config['lora_alpha']}")
        if 'target_modules' in config:
            print(f"   ç›®æ ‡æ¨¡å—: {config['target_modules']}")
        
        return config.get('base_model_name_or_path')
    else:
        print("âŒ æœªæ‰¾åˆ°adapter_config.jsonæ–‡ä»¶")
        return None


def main():
    parser = argparse.ArgumentParser(
        description="PEFTæ¨¡å‹åˆå¹¶å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åˆå¹¶LoRAé€‚é…å™¨
  python merge_peft_model.py --base_model Qwen/Qwen2.5-7B-Instruct --adapter_path ./outputs/sft_lora/checkpoint-1000 --output_path ./merged_model
  
  # éªŒè¯åˆå¹¶åçš„æ¨¡å‹
  python merge_peft_model.py --validate ./merged_model
  
  # æ£€æŸ¥é€‚é…å™¨ä¿¡æ¯
  python merge_peft_model.py --check_adapter ./outputs/sft_lora/checkpoint-1000
"""
    )
    
    parser.add_argument(
        "--base_model",
        type=str,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹å (å¦‚: Qwen/Qwen2.5-7B-Instruct)"
    )
    parser.add_argument(
        "--adapter_path",
        type=str,
        help="LoRAé€‚é…å™¨è·¯å¾„ (å¦‚: ./outputs/sft_lora/checkpoint-1000)"
    )
    parser.add_argument(
        "--output_path",
        type=str,
        help="åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ (å¦‚: ./merged_model)"
    )
    parser.add_argument(
        "--validate",
        type=str,
        help="éªŒè¯æŒ‡å®šè·¯å¾„çš„æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"
    )
    parser.add_argument(
        "--check_adapter",
        type=str,
        help="æ£€æŸ¥æŒ‡å®šè·¯å¾„çš„é€‚é…å™¨ä¿¡æ¯"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["cpu", "cuda", "auto"],
        help="è®¾å¤‡é€‰æ‹© (é»˜è®¤: auto)"
    )
    parser.add_argument(
        "--torch_dtype",
        type=str,
        default="float16",
        choices=["float16", "bfloat16", "float32"],
        help="PyTorchæ•°æ®ç±»å‹ (é»˜è®¤: float16)"
    )
    parser.add_argument(
        "--sample_text",
        type=str,
        default="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚",
        help="éªŒè¯æ—¶ä½¿ç”¨çš„æµ‹è¯•æ–‡æœ¬"
    )
    
    args = parser.parse_args()
    
    try:
        if args.validate:
            # éªŒè¯æ¨¡å‹
            validate_model(args.validate, args.sample_text)
        
        elif args.check_adapter:
            # æ£€æŸ¥é€‚é…å™¨ä¿¡æ¯
            base_model = check_adapter_info(args.check_adapter)
            if base_model:
                print(f"\nğŸ’¡ å»ºè®®çš„åˆå¹¶å‘½ä»¤:")
                print(f"python merge_peft_model.py --base_model {base_model} --adapter_path {args.check_adapter} --output_path ./merged_model")
        
        else:
            # åˆå¹¶æ¨¡å‹
            if not args.base_model or not args.adapter_path or not args.output_path:
                print("âŒ åˆå¹¶æ“ä½œéœ€è¦æŒ‡å®š --base_model, --adapter_path å’Œ --output_path")
                print("ä½¿ç”¨ --help æŸ¥çœ‹è¯¦ç»†ç”¨æ³•")
                sys.exit(1)
            
            # æ£€æŸ¥é€‚é…å™¨è·¯å¾„
            if not os.path.exists(args.adapter_path):
                print(f"âŒ é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")
                sys.exit(1)
            
            # æ£€æŸ¥é€‚é…å™¨ä¿¡æ¯
            print("\n" + "="*50)
            base_model_from_config = check_adapter_info(args.adapter_path)
            
            if base_model_from_config and base_model_from_config != args.base_model:
                print(f"âš ï¸  è­¦å‘Š: é€‚é…å™¨çš„åŸºç¡€æ¨¡å‹ ({base_model_from_config}) ä¸æŒ‡å®šçš„åŸºç¡€æ¨¡å‹ ({args.base_model}) ä¸åŒ¹é…")
                response = input("æ˜¯å¦ç»§ç»­? (y/N): ")
                if response.lower() != 'y':
                    print("æ“ä½œå·²å–æ¶ˆ")
                    sys.exit(0)
            
            print("="*50 + "\n")
            
            # æ‰§è¡Œåˆå¹¶
            merge_peft_model(
                args.base_model,
                args.adapter_path,
                args.output_path,
                args.device,
                args.torch_dtype
            )
            
            # è¯¢é—®æ˜¯å¦éªŒè¯
            response = input("\næ˜¯å¦éªŒè¯åˆå¹¶åçš„æ¨¡å‹? (Y/n): ")
            if response.lower() != 'n':
                print("\n" + "="*50)
                validate_model(args.output_path, args.sample_text)
    
    except KeyboardInterrupt:
        print("\næ“ä½œå·²å–æ¶ˆ")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()