# PEFTæ¨¡å‹ä½¿ç”¨å®Œæ•´æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨æœ¬é¡¹ç›®ä¸­ä½¿ç”¨PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬LoRAå’ŒQLoRAçš„é…ç½®ã€è®­ç»ƒã€ä¿å­˜å’Œåˆå¹¶ã€‚

## ğŸ“‹ ç›®å½•

1. [PEFTç®€ä»‹](#peftç®€ä»‹)
2. [é…ç½®PEFT](#é…ç½®peft)
3. [è®­ç»ƒPEFTæ¨¡å‹](#è®­ç»ƒpeftæ¨¡å‹)
4. [ä¿å­˜PEFTé€‚é…å™¨](#ä¿å­˜pefté€‚é…å™¨)
5. [åˆå¹¶PEFTæ¨¡å‹](#åˆå¹¶peftæ¨¡å‹)
6. [ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹](#ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
8. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## ğŸ¯ PEFTç®€ä»‹

### ä»€ä¹ˆæ˜¯PEFTï¼Ÿ

PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œåªè®­ç»ƒæ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚

### æ”¯æŒçš„PEFTæ–¹æ³•

| æ–¹æ³• | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒé€Ÿåº¦ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|------|----------|
| **LoRA** | ä¸­ç­‰ (8-16GB) | å¿« | ä¼˜ç§€ | å¹³è¡¡æ€§èƒ½å’Œèµ„æº |
| **QLoRA** | ä½ (4-8GB) | ä¸­ç­‰ | è‰¯å¥½ | æ˜¾å­˜å—é™ç¯å¢ƒ |
| **å…¨é‡å¾®è°ƒ** | é«˜ (16-32GB) | æ…¢ | æœ€ä½³ | èµ„æºå……è¶³æ—¶ |

## âš™ï¸ é…ç½®PEFT

### 1. LoRAé…ç½®

åœ¨ `config.yaml` ä¸­å¯ç”¨LoRAï¼š

```yaml
model:
  model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"
  use_peft: true
  peft_config:
    r: 16                    # LoRA rankï¼Œæ§åˆ¶é€‚é…å™¨å¤§å°
    lora_alpha: 32          # LoRAç¼©æ”¾å‚æ•°ï¼Œé€šå¸¸æ˜¯rçš„2å€
    lora_dropout: 0.1       # Dropoutç‡
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]  # ç›®æ ‡æ¨¡å—
    bias: "none"            # åç½®å¤„ç†: "none", "all", "lora_only"
  # ä¸è®¾ç½®quantization_config
```

### 2. QLoRAé…ç½®

åœ¨ `config.yaml` ä¸­å¯ç”¨QLoRAï¼ˆ4bité‡åŒ– + LoRAï¼‰ï¼š

```yaml
model:
  model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"
  use_peft: true
  peft_config:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    bias: "none"
  
  # æ·»åŠ é‡åŒ–é…ç½®å¯ç”¨QLoRA
  quantization_config:
    load_in_4bit: true
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_use_double_quant: true
```

### 3. å…¨é‡å¾®è°ƒé…ç½®

```yaml
model:
  model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"
  use_peft: false  # å…³é—­PEFT
  # ä¸éœ€è¦peft_configå’Œquantization_config
```

### å‚æ•°è¯´æ˜

#### LoRAå‚æ•°

- **r (rank)**: LoRAé€‚é…å™¨çš„ç§©
  - å€¼è¶Šå¤§ï¼Œé€‚é…å™¨å‚æ•°è¶Šå¤šï¼Œæ•ˆæœå¯èƒ½æ›´å¥½ä½†æ˜¾å­˜å ç”¨æ›´é«˜
  - æ¨èå€¼: 8, 16, 32, 64
  - ä¸€èˆ¬ä»16å¼€å§‹å°è¯•

- **lora_alpha**: LoRAç¼©æ”¾å‚æ•°
  - æ§åˆ¶LoRAé€‚é…å™¨çš„å­¦ä¹ ç‡ç¼©æ”¾
  - é€šå¸¸è®¾ä¸ºrçš„2å€
  - å¦‚æœr=16ï¼Œåˆ™lora_alpha=32

- **lora_dropout**: Dropoutç‡
  - é˜²æ­¢è¿‡æ‹Ÿåˆ
  - æ¨èå€¼: 0.05-0.1

- **target_modules**: åº”ç”¨LoRAçš„æ¨¡å—
  - æ³¨æ„åŠ›å±‚: `["q_proj", "k_proj", "v_proj", "o_proj"]`
  - æ‰€æœ‰çº¿æ€§å±‚: `["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]`

## ğŸš€ è®­ç»ƒPEFTæ¨¡å‹

### 1. SFTè®­ç»ƒï¼ˆLoRAï¼‰

```bash
# ä½¿ç”¨LoRAè¿›è¡ŒSFTè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm sft --experiment_name "sft_qwen_lora_r16"
```

### 2. Rewardæ¨¡å‹è®­ç»ƒï¼ˆQLoRAï¼‰

```bash
# ä½¿ç”¨QLoRAè®­ç»ƒRewardæ¨¡å‹
python main.py --config config_qlora.yaml --mode train --algorithm reward --experiment_name "reward_qwen_qlora"
```

### 3. RLHFè®­ç»ƒï¼ˆLoRAï¼‰

```bash
# ä½¿ç”¨LoRAè¿›è¡ŒPPOè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm ppo --experiment_name "ppo_qwen_lora"
```

## ğŸ’¾ ä¿å­˜PEFTé€‚é…å™¨

### è®­ç»ƒè¿‡ç¨‹ä¸­çš„è‡ªåŠ¨ä¿å­˜

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒPEFTé€‚é…å™¨ä¼šè‡ªåŠ¨ä¿å­˜åˆ°è¾“å‡ºç›®å½•ï¼š

```
outputs/
â””â”€â”€ sft_qwen_lora_r16/
    â”œâ”€â”€ checkpoint-500/
    â”‚   â”œâ”€â”€ adapter_config.json    # é€‚é…å™¨é…ç½®
    â”‚   â”œâ”€â”€ adapter_model.bin      # é€‚é…å™¨æƒé‡
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ checkpoint-1000/
    â””â”€â”€ final_model/
```

### æ‰‹åŠ¨ä¿å­˜é€‚é…å™¨

ä½¿ç”¨æˆ‘ä»¬æä¾›çš„è„šæœ¬ï¼š

```bash
# ä¿å­˜PEFTé€‚é…å™¨
python scripts/peft_model_manager.py --action save --model_path ./outputs/sft_lora/final_model --output_path ./saved_adapter
```

## ğŸ”„ åˆå¹¶PEFTæ¨¡å‹

### ä¸ºä»€ä¹ˆéœ€è¦åˆå¹¶ï¼Ÿ

- **PEFTé€‚é…å™¨**: åªåŒ…å«å°‘é‡å‚æ•°ï¼Œéœ€è¦ä¸åŸºç¡€æ¨¡å‹ä¸€èµ·ä½¿ç”¨
- **åˆå¹¶åçš„æ¨¡å‹**: åŒ…å«å®Œæ•´å‚æ•°ï¼Œå¯ä»¥ç‹¬ç«‹ä½¿ç”¨ï¼Œä¾¿äºéƒ¨ç½²

### ä½¿ç”¨ç®€åŒ–è„šæœ¬åˆå¹¶

æˆ‘ä»¬æä¾›äº†ç®€åŒ–çš„åˆå¹¶è„šæœ¬ `merge_peft_model.py`ï¼š

```bash
# 1. æ£€æŸ¥é€‚é…å™¨ä¿¡æ¯
python merge_peft_model.py --check_adapter ./outputs/sft_lora/checkpoint-1000

# 2. åˆå¹¶LoRAé€‚é…å™¨
python merge_peft_model.py \
    --base_model Qwen/Qwen2.5-7B-Instruct \
    --adapter_path ./outputs/sft_lora/checkpoint-1000 \
    --output_path ./merged_model

# 3. éªŒè¯åˆå¹¶åçš„æ¨¡å‹
python merge_peft_model.py --validate ./merged_model
```

### ä½¿ç”¨é«˜çº§è„šæœ¬åˆå¹¶

```bash
# ä½¿ç”¨é«˜çº§è„šæœ¬è¿›è¡Œåˆå¹¶
python scripts/peft_model_manager.py \
    --action merge \
    --base_model Qwen/Qwen2.5-7B-Instruct \
    --adapter_path ./outputs/sft_lora/checkpoint-1000 \
    --output_path ./merged_model \
    --torch_dtype float16
```

### åˆå¹¶è¿‡ç¨‹è¯´æ˜

1. **åŠ è½½åŸºç¡€æ¨¡å‹**: ä»HuggingFaceæˆ–æœ¬åœ°è·¯å¾„åŠ è½½åŸå§‹æ¨¡å‹
2. **åŠ è½½é€‚é…å™¨**: åŠ è½½è®­ç»ƒå¥½çš„LoRA/QLoRAé€‚é…å™¨
3. **åˆå¹¶æƒé‡**: å°†é€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­
4. **ä¿å­˜å®Œæ•´æ¨¡å‹**: ä¿å­˜ä¸ºæ ‡å‡†çš„HuggingFaceæ¨¡å‹æ ¼å¼

## ğŸ® ä½¿ç”¨åˆå¹¶åçš„æ¨¡å‹

### 1. åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½åˆå¹¶åçš„æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "./merged_model",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("./merged_model")

# ç”Ÿæˆæ–‡æœ¬
input_text = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(
    inputs.input_ids,
    max_length=100,
    do_sample=True,
    temperature=0.7
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

### 2. ä½¿ç”¨vLLMéƒ¨ç½²

```bash
# å®‰è£…vLLM
pip install vllm

# å¯åŠ¨APIæœåŠ¡
python -m vllm.entrypoints.openai.api_server \
    --model ./merged_model \
    --served-model-name my-model \
    --host 0.0.0.0 \
    --port 8000
```

### 3. ä¸Šä¼ åˆ°HuggingFace Hub

```python
from huggingface_hub import HfApi

# ä¸Šä¼ æ¨¡å‹
api = HfApi()
api.upload_folder(
    folder_path="./merged_model",
    repo_id="your-username/your-model-name",
    repo_type="model"
)
```

## â“ å¸¸è§é—®é¢˜

### Q1: LoRAå’ŒQLoRAçš„åŒºåˆ«ï¼Ÿ

**A**: 
- **LoRA**: åªä½¿ç”¨ä½ç§©é€‚é…å™¨ï¼Œä¸è¿›è¡Œé‡åŒ–
- **QLoRA**: LoRA + 4bité‡åŒ–ï¼Œæ˜¾å­˜éœ€æ±‚æ›´ä½

### Q2: å¦‚ä½•é€‰æ‹©LoRAçš„rankå€¼ï¼Ÿ

**A**: 
- å°æ¨¡å‹(7Bä»¥ä¸‹): r=8-16
- ä¸­ç­‰æ¨¡å‹(7B-13B): r=16-32
- å¤§æ¨¡å‹(13Bä»¥ä¸Š): r=32-64
- ä»16å¼€å§‹å°è¯•ï¼Œæ ¹æ®æ•ˆæœè°ƒæ•´

### Q3: è®­ç»ƒååªæœ‰é€‚é…å™¨æ–‡ä»¶ï¼Œå¦‚ä½•ä½¿ç”¨ï¼Ÿ

**A**: æœ‰ä¸¤ç§æ–¹å¼ï¼š
1. **ç›´æ¥ä½¿ç”¨**: åŠ è½½åŸºç¡€æ¨¡å‹ + é€‚é…å™¨
2. **åˆå¹¶åä½¿ç”¨**: ä½¿ç”¨æœ¬æŒ‡å—çš„åˆå¹¶è„šæœ¬

### Q4: åˆå¹¶åçš„æ¨¡å‹å¤§å°ï¼Ÿ

**A**: åˆå¹¶åçš„æ¨¡å‹å¤§å°ä¸åŸå§‹åŸºç¡€æ¨¡å‹ç›¸åŒï¼Œå› ä¸ºLoRAå‚æ•°è¢«åˆå¹¶åˆ°åŸå§‹æƒé‡ä¸­ã€‚

### Q5: å¯ä»¥åœ¨ä¸åŒçš„åŸºç¡€æ¨¡å‹ä¹‹é—´è½¬ç§»é€‚é…å™¨å—ï¼Ÿ

**A**: ä¸å»ºè®®ã€‚é€‚é…å™¨æ˜¯é’ˆå¯¹ç‰¹å®šåŸºç¡€æ¨¡å‹è®­ç»ƒçš„ï¼Œåœ¨ä¸åŒæ¨¡å‹é—´è½¬ç§»å¯èƒ½æ•ˆæœä¸ä½³ã€‚

### Q6: å¦‚ä½•éªŒè¯PEFTè®­ç»ƒæ˜¯å¦æˆåŠŸï¼Ÿ

**A**: 
1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—ä¸­çš„LoRAä¿¡æ¯
2. ç¡®è®¤è¾“å‡ºç›®å½•ä¸­æœ‰ `adapter_config.json` å’Œ `adapter_model.bin`
3. ä½¿ç”¨éªŒè¯è„šæœ¬æµ‹è¯•æ¨¡å‹

## ğŸ† æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„PEFTæ–¹æ³•

- **æ˜¾å­˜å……è¶³(16GB+)**: ä¼˜å…ˆé€‰æ‹©LoRA
- **æ˜¾å­˜å—é™(8GB-)**: é€‰æ‹©QLoRA
- **è¿½æ±‚æœ€ä½³æ•ˆæœ**: å…¨é‡å¾®è°ƒï¼ˆå¦‚æœèµ„æºå…è®¸ï¼‰

### 2. å‚æ•°è°ƒä¼˜å»ºè®®

```yaml
# ä¿å®ˆé…ç½®ï¼ˆç¨³å®šä½†å¯èƒ½æ•ˆæœä¸€èˆ¬ï¼‰
peft_config:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1

# å¹³è¡¡é…ç½®ï¼ˆæ¨èï¼‰
peft_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1

# æ¿€è¿›é…ç½®ï¼ˆæ•ˆæœå¯èƒ½æ›´å¥½ä½†éœ€è¦æ›´å¤šèµ„æºï¼‰
peft_config:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.05
```

### 3. ç›®æ ‡æ¨¡å—é€‰æ‹©

```yaml
# æœ€å°é…ç½®ï¼ˆåªå¯¹æ³¨æ„åŠ›å±‚åº”ç”¨LoRAï¼‰
target_modules: ["q_proj", "v_proj"]

# æ ‡å‡†é…ç½®ï¼ˆæ¨èï¼‰
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

# å®Œæ•´é…ç½®ï¼ˆåŒ…å«FFNå±‚ï¼‰
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
```

### 4. å®éªŒç®¡ç†

```bash
# ä½¿ç”¨æœ‰æ„ä¹‰çš„å®éªŒåç§°
python main.py --experiment_name "sft_qwen7b_lora_r16_alpha32_$(date +%Y%m%d)"

# ä¿å­˜é…ç½®æ–‡ä»¶
cp config.yaml outputs/sft_qwen7b_lora_r16_alpha32_20241201/config_used.yaml
```

### 5. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

```
models/
â”œâ”€â”€ base_models/           # åŸºç¡€æ¨¡å‹
â”‚   â””â”€â”€ Qwen2.5-7B-Instruct/
â”œâ”€â”€ adapters/             # PEFTé€‚é…å™¨
â”‚   â”œâ”€â”€ sft_lora_v1/
â”‚   â”œâ”€â”€ reward_qlora_v1/
â”‚   â””â”€â”€ ppo_lora_v1/
â””â”€â”€ merged_models/        # åˆå¹¶åçš„æ¨¡å‹
    â”œâ”€â”€ sft_merged_v1/
    â””â”€â”€ ppo_merged_v1/
```

### 6. æ€§èƒ½ç›‘æ§

è®­ç»ƒæ—¶å…³æ³¨ä»¥ä¸‹æŒ‡æ ‡ï¼š
- **å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹**: åº”è¯¥è¿œå°äº100%
- **æ˜¾å­˜ä½¿ç”¨**: QLoRA < LoRA < å…¨é‡å¾®è°ƒ
- **è®­ç»ƒé€Ÿåº¦**: LoRA > QLoRA > å…¨é‡å¾®è°ƒ
- **æ”¶æ•›æƒ…å†µ**: æŸå¤±æ˜¯å¦æ­£å¸¸ä¸‹é™

## ğŸ“š ç›¸å…³èµ„æº

- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [QLoRAè®ºæ–‡](https://arxiv.org/abs/2305.14314)
- [PEFTåº“æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

1. **ImportError: No module named 'peft'**
   ```bash
   pip install peft>=0.4.0
   ```

2. **CUDA out of memory**
   - å‡å°batch_size
   - ä½¿ç”¨QLoRAè€Œä¸æ˜¯LoRA
   - å‡å°LoRAçš„rankå€¼

3. **é€‚é…å™¨åŠ è½½å¤±è´¥**
   - æ£€æŸ¥adapter_config.jsonæ˜¯å¦å­˜åœ¨
   - ç¡®è®¤åŸºç¡€æ¨¡å‹è·¯å¾„æ­£ç¡®
   - éªŒè¯é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹çš„å…¼å®¹æ€§

4. **åˆå¹¶åæ¨¡å‹æ•ˆæœå·®**
   - æ£€æŸ¥è®­ç»ƒæ˜¯å¦å……åˆ†æ”¶æ•›
   - éªŒè¯é€‚é…å™¨é…ç½®æ˜¯å¦åˆç†
   - å°è¯•è°ƒæ•´LoRAå‚æ•°

---

å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹é¡¹ç›®çš„å…¶ä»–æ–‡æ¡£æˆ–æäº¤Issueã€‚