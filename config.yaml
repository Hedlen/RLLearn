# RL Learning Framework Configuration

# Model Configuration
model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"  # Qwen2.5-3B-Instruct model
  model_name_or_path: "Qwen/Qwen2.5-1.5B-Instruct"  # HuggingFace model path
  tokenizer_name_or_path: null  # 默认使用与模型相同的tokenizer
  max_length: 2048  # Qwen模型支持更长的序列长度
  use_peft: true
  peft_config:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # Qwen2.5模型的注意力和MLP层

# Training Configuration
training:
  output_dir: "./outputs"
  experiment_name: null  # 实验名称，如果设置则会在output_dir下创建对应的子目录
  num_epochs: 3  # 修正参数名
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 0.00005
  warmup_steps: 100
  warmup_ratio: 0.0
  max_grad_norm: 1.0
  weight_decay: 0.01
  
  # Scheduler
  lr_scheduler_type: "linear"
  
  # Evaluation and saving
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  evaluation_strategy: "steps"  # "steps", "epoch", "no"
  save_strategy: "steps"  # "steps", "epoch", "no"
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.0
  
  # Mixed precision
  fp16: true
  bf16: false
  
  # Optimization
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 0.00000001
  
  # DataLoader
  dataloader_num_workers: 4
  dataloader_drop_last: false
  dataloader_pin_memory: true
  remove_unused_columns: false
  
  # Reproducibility
  seed: 42
  
  # Resume training
  resume_from_checkpoint: null
  
  # Logging
  logging_dir: null  # 默认为 output_dir/logs
  report_to: ["tensorboard"]

# Algorithm Specific Configurations
algorithms:
  # SFT (Supervised Fine-Tuning) Configuration
  sft:
    # Data processing
    max_length: 2048
    truncation_side: "right"  # "left", "right"
    padding_side: "right"     # "left", "right"
    
    # Training specific
    gradient_checkpointing: false
    dataloader_drop_last: true
    label_smoothing: 0.0
    
    # Loss computation
    ignore_index: -100
    loss_type: "cross_entropy"  # "cross_entropy", "focal"
    focal_alpha: 1.0
    focal_gamma: 2.0
    
    # Model specific
    freeze_base_model: false
    freeze_layers: null  # List of layer indices to freeze
    
    # Evaluation
    eval_steps: 500
    eval_accumulation_steps: null
    
    # Generation for evaluation
    eval_generation: true
    eval_max_new_tokens: 128
    eval_temperature: 1.0
    eval_top_k: 50
    eval_top_p: 0.95
    eval_do_sample: true
  
  # Reward Model Training Configuration
  reward:
    # Reward model specific hyperparameters
    margin: 0.0
    loss_type: "ranking"  # "ranking", "regression", "classification"
    ranking_loss_type: "hinge"  # "hinge", "log_sigmoid", "cross_entropy"
    label_smoothing: 0.0
    
    # Data processing
    max_length: 512
    truncation_side: "right"  # "left", "right"
    
    # Training specific
    gradient_checkpointing: false
    dataloader_drop_last: true
    
    # Evaluation
    eval_steps: 500
    eval_accumulation_steps: null
    
    # Model specific
    freeze_base_model: false
    freeze_layers: null  # List of layer indices to freeze
  
  # PPO Configuration
  ppo:
    learning_rate: 0.0000141
    mini_batch_size: 1
    batch_size: 8
    ppo_epochs: 4
    gamma: 1.0
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    vf_coef: 0.1
    ent_coef: 0.0
    target_kl: 0.1
    max_grad_norm: 0.5
    
  # DPO Configuration
  dpo:
    beta: 0.1
    learning_rate: 0.0000005
    max_length: 2048
    max_prompt_length: 1024
    label_smoothing: 0.0
    loss_type: "sigmoid"  # or "hinge", "ipo"
    
  # GRPO Configuration
  grpo:
    learning_rate: 0.00001
    group_size: 8
    temperature: 1.0
    kl_penalty: 0.1

# Data Configuration
data:
  # Dataset source (choose one)
  dataset_name: null  # HuggingFace dataset name
  dataset_config: null  # Dataset configuration
  
  # Multiple datasets configuration (支持多个数据集)
  datasets:
    sft:
      # SFT训练数据集列表
      train_files:
        - path: "./data/sft_train.json"
          weight: 1.0  # 数据集权重，用于采样
          name: "sft_sample_data"  # 数据集名称
        # 可以添加更多SFT数据集
        # - path: "./data/another_sft.json"
        #   weight: 0.5
        #   name: "another_dataset"
      
      # 验证数据集列表 (支持多个验证数据集自动合并)
      validation_files:
        - path: "./data/eval.json"
          weight: 1.0  # 验证集权重，用于合并时的采样比例
          name: "sft_sample_eval"
        # 可以添加更多验证数据集
        # - path: "./data/another_eval.json"
        #   weight: 0.8
        #   name: "another_eval"
      
      # 是否合并所有训练数据集
      merge_datasets: true
      # 合并后的缓存文件路径
      merged_cache_path: "./cache/merged_sft_train.json"
      # 合并后的验证集缓存路径
      merged_validation_cache_path: "./cache/merged_sft_validation.json"
    
    reward:
      # 奖励模型训练数据集
      train_files:
        - path: "./data/preference_train.json"
          weight: 1.0
          name: "preference_data"
      
      # 验证数据集列表 (支持多个验证数据集自动合并)
      validation_files:
        - path: "./data/preference_eval.json"
          weight: 1.0  # 验证集权重，用于合并时的采样比例
          name: "preference_eval"
        # 可以添加更多验证数据集
        # - path: "./data/another_preference_eval.json"
        #   weight: 0.9
        #   name: "another_preference_eval"
      
      merge_datasets: true
      merged_cache_path: "./cache/merged_reward_train.json"
      # 合并后的验证集缓存路径
      merged_validation_cache_path: "./cache/merged_reward_validation.json"
    
    rlhf:
      # RLHF训练数据集（PPO/DPO等）
      train_files:
        - path: "./data/rlhf_train.json"
          weight: 1.0
          name: "rlhf_data"
      
      # 验证数据集列表 (支持多个验证数据集自动合并)
      validation_files:
        - path: "./data/rlhf_eval.json"
          weight: 1.0  # 验证集权重，用于合并时的采样比例
          name: "rlhf_eval"
        # 可以添加更多验证数据集
        # - path: "./data/another_rlhf_eval.json"
        #   weight: 1.0
        #   name: "another_rlhf_eval"
      
      merge_datasets: false
      merged_cache_path: "./cache/merged_rlhf_train.json"
      # 合并后的验证集缓存路径
      merged_validation_cache_path: "./cache/merged_rlhf_validation.json"
  
  # Legacy single file configuration (向后兼容)
  train_file: null  # 如果设置，将覆盖datasets配置
  validation_file: null
  test_file: null
  
  # Data processing
  max_train_samples: null  # 限制训练样本数量
  max_eval_samples: null   # 限制评估样本数量
  preprocessing_num_workers: 1
  overwrite_cache: false
  validation_split_percentage: 10
  max_length: 512  # 最大序列长度
  cache_dir: "./cache"  # 缓存目录
  
  # Data format
  data_format: "json"  # "json", "jsonl", "csv"
  text_column: "text"  # 文本列名
  label_column: "label"  # 标签列名（用于分类任务）
  
  # For preference data (reward model training)
  chosen_column: "chosen"  # 偏好数据中的选择列
  rejected_column: "rejected"  # 偏好数据中的拒绝列
  
  # For conversation data
  conversation_column: "conversation"  # 对话列名
  prompt_column: "prompt"  # 提示列名
  response_column: "response"  # 回复列名
  
  # Data merging configuration
  merge_config:
    # 数据合并策略
    strategy: "weighted_sampling"  # "concat", "weighted_sampling", "balanced"
    # 随机种子
    seed: 42
    # 是否打乱合并后的数据
    shuffle: true
    # 最大合并样本数
    max_merged_samples: null
    # 是否保存合并统计信息
    save_merge_stats: true

# RLHF Configuration
rlhf:
  # Model paths for RLHF pipeline
  reward_model_path: "./outputs/reward_model"  # 奖励模型路径
  sft_model_path: "./outputs/sft_model"        # SFT模型路径
  ref_model_path: null                          # 参考模型路径（默认使用SFT模型）
  
  # Score processing
  use_score_scaling: true   # 是否使用分数缩放
  use_score_norm: true      # 是否使用分数归一化
  score_clip: 5.0           # 分数裁剪阈值
  
  # KL divergence settings
  kl_coef: 0.1              # KL散度系数
  adaptive_kl: false        # 是否使用自适应KL
  target_kl: 6.0            # 目标KL散度
  
  # Generation settings for RL training
  max_new_tokens: 1024       # RL训练时的最大生成长度
  temperature: 1.0          # 生成温度
  top_k: 0                  # Top-k采样（0表示不使用）
  top_p: 1.0                # Top-p采样
  do_sample: true           # 是否使用采样

# 评估配置
evaluation:
  eval_dataset: null  # "./data/eval.json"  # 评估数据集（需要用户提供）
  eval_batch_size: 8                        # 评估批大小
  eval_steps: 500                           # 评估间隔
  max_eval_samples: 1000                    # 最大评估样本数
  
  # 生成参数
  generation:
    max_new_tokens: 1024                     # 最大生成长度
    temperature: 0.7                        # 生成温度
    top_p: 0.9                              # Top-p采样
    top_k: 50                               # Top-k采样
    do_sample: true                         # 是否采样
    repetition_penalty: 1.1                 # 重复惩罚
    length_penalty: 1.0                     # 长度惩罚
  
  # 评估指标
  metrics:
    compute_bleu: true                      # 计算BLEU分数
    compute_rouge: true                     # 计算ROUGE分数
    compute_diversity: true                 # 计算多样性指标
    compute_perplexity: true                # 计算困惑度
    compute_reward: true                    # 计算奖励分数
  
  # 输出设置
  save_generations: true                    # 保存生成结果
  save_metrics: true                        # 保存评估指标
  output_dir: "./eval_results"              # 评估结果输出目录

# Logging and Monitoring
logging:
  use_wandb: false
  wandb_project: "rl-learning"
  wandb_run_name: null
  use_tensorboard: true
  log_level: "INFO"

# Hardware Configuration
hardware:
  use_cuda: true
  device_map: "auto"
  torch_dtype: "float16"
  use_deepspeed: false
  deepspeed_config: null