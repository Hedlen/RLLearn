# RL Learning Framework Configuration

# Model Configuration
model:
  name: "gpt2"  # or llama2, chatglm, etc.
  model_name_or_path: "gpt2"
  tokenizer_name_or_path: null
  max_length: 512
  use_peft: true
  peft_config:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["c_attn", "c_proj"]

# Training Configuration
training:
  output_dir: "./outputs"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  dataloader_num_workers: 4
  remove_unused_columns: false
  fp16: true
  gradient_checkpointing: true

# Algorithm Specific Configurations
algorithms:
  ppo:
    learning_rate: 1.41e-5
    mini_batch_size: 1
    batch_size: 8
    ppo_epochs: 4
    gamma: 1.0
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    vf_coef: 0.1
    ent_coef: 0.0
    target_kl: 0.1
    max_grad_norm: 0.5
    
  dpo:
    beta: 0.1
    learning_rate: 5e-7
    max_length: 512
    max_prompt_length: 256
    label_smoothing: 0.0
    loss_type: "sigmoid"  # or "hinge", "ipo"
    
  grpo:
    learning_rate: 1e-5
    group_size: 8
    temperature: 1.0
    kl_penalty: 0.1

# Data Configuration
data:
  dataset_name: null
  dataset_config: null
  train_file: null
  validation_file: null
  test_file: null
  max_train_samples: null
  max_eval_samples: null
  preprocessing_num_workers: 4
  overwrite_cache: false
  validation_split_percentage: 10

# RLHF Configuration
rlhf:
  reward_model_path: null
  sft_model_path: null
  ref_model_path: null
  use_score_scaling: true
  use_score_norm: true
  score_clip: 5.0

# Evaluation Configuration
evaluation:
  eval_dataset: null
  eval_batch_size: 8
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  do_sample: true
  num_return_sequences: 1

# Logging and Monitoring
logging:
  use_wandb: false
  wandb_project: "rl-learning"
  wandb_run_name: null
  use_tensorboard: true
  log_level: "INFO"

# Hardware Configuration
hardware:
  use_cuda: true
  device_map: "auto"
  torch_dtype: "float16"
  use_deepspeed: false
  deepspeed_config: null