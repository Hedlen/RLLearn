# å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ (RL Learning Framework)

ä¸€ä¸ªä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒPPOã€DPOã€GRPOç­‰ä¸»æµç®—æ³•ï¼Œé€‚ç”¨äºRLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰åœºæ™¯ã€‚

## ğŸš€ ç‰¹æ€§

- **å¤šç®—æ³•æ”¯æŒ**: PPOã€DPOã€GRPOç­‰å¼ºåŒ–å­¦ä¹ ç®—æ³•
- **æ¨¡å—åŒ–è®¾è®¡**: çµæ´»çš„ç»„ä»¶æ¶æ„ï¼Œæ˜“äºæ‰©å±•å’Œå®šåˆ¶
- **å®Œæ•´è®­ç»ƒæµç¨‹**: ä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹è®­ç»ƒçš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆ
- **å¤šç§æ¨¡å‹**: ç­–ç•¥æ¨¡å‹ã€ä»·å€¼æ¨¡å‹ã€å¥–åŠ±æ¨¡å‹
- **ä¸°å¯Œçš„æ•°æ®å¤„ç†**: æ”¯æŒSFTã€åå¥½å­¦ä¹ ã€å¯¹è¯ç­‰å¤šç§æ•°æ®æ ¼å¼
- **åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ**: æ”¯æŒå•èŠ‚ç‚¹å¤šGPUå’Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ
  - **DDP (DistributedDataParallel)**: PyTorchåŸç”Ÿåˆ†å¸ƒå¼è®­ç»ƒ
  - **DeepSpeed**: æ”¯æŒZeROä¼˜åŒ–å™¨ï¼Œå†…å­˜é«˜æ•ˆè®­ç»ƒå¤§æ¨¡å‹
  - **è‡ªåŠ¨ç­–ç•¥é€‰æ‹©**: æ ¹æ®æ¨¡å‹å¤§å°è‡ªåŠ¨é€‰æ‹©æœ€ä½³åˆ†å¸ƒå¼ç­–ç•¥
- **å®æ—¶ç›‘æ§**: TensorBoardé›†æˆï¼Œå®æ—¶è®­ç»ƒç›‘æ§
- **æ£€æŸ¥ç‚¹ç®¡ç†**: è‡ªåŠ¨ä¿å­˜å’Œæ¢å¤è®­ç»ƒçŠ¶æ€

## ğŸ“ é¡¹ç›®ç»“æ„

```
rl_learning/
â”œâ”€â”€ config.yaml              # ä¸»é…ç½®æ–‡ä»¶
â”œâ”€â”€ config_deepspeed_example.yaml # DeepSpeedé…ç½®ç¤ºä¾‹
â”œâ”€â”€ config_distributed_example.yaml # åˆ†å¸ƒå¼è®­ç»ƒé…ç½®ç¤ºä¾‹
â”œâ”€â”€ config_examples.yaml     # é…ç½®ç¤ºä¾‹æ–‡ä»¶
â”œâ”€â”€ config_rlhf_example.yaml # RLHFé…ç½®ç¤ºä¾‹
â”œâ”€â”€ main.py                   # ç”Ÿäº§çº§è®­ç»ƒæ¡†æ¶ä¸»å…¥å£
â”œâ”€â”€ example_training.py       # å®Œæ•´è®­ç»ƒç¤ºä¾‹è„šæœ¬
â”œâ”€â”€ prepare_datasets.py       # æ•°æ®é›†å‡†å¤‡è„šæœ¬
â”œâ”€â”€ merge_datasets.py         # æ•°æ®é›†åˆå¹¶è„šæœ¬
â”œâ”€â”€ merge_peft_model.py       # PEFTæ¨¡å‹åˆå¹¶è„šæœ¬
â”œâ”€â”€ quick_test.py            # å¿«é€Ÿæµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_models.py           # æ¨¡å‹æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_training.py         # è®­ç»ƒæµ‹è¯•è„šæœ¬
â”œâ”€â”€ requirements.txt          # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ README.md                # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ TRAINING_MODES.md        # è®­ç»ƒæ¨¡å¼è¯´æ˜
â”œâ”€â”€ LICENSE                  # å¼€æºåè®®
â”œâ”€â”€ .gitignore              # Gitå¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ deepspeed/         # DeepSpeedé…ç½®
â”œâ”€â”€ scripts/               # è„šæœ¬æ–‡ä»¶
â”‚   â”œâ”€â”€ run_distributed.py # åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨
â”‚   â”œâ”€â”€ check_environment.py # ç¯å¢ƒæ£€æŸ¥
â”‚   â”œâ”€â”€ validate_config.py # é…ç½®éªŒè¯
â”‚   â”œâ”€â”€ training_monitor.py # è®­ç»ƒç›‘æ§
â”‚   â”œâ”€â”€ pre_training_check.py # è®­ç»ƒå‰æ£€æŸ¥
â”‚   â”œâ”€â”€ peft_model_manager.py # PEFTæ¨¡å‹ç®¡ç†
â”‚   â””â”€â”€ training_utils.py   # è®­ç»ƒå·¥å…·
â”œâ”€â”€ data/                  # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ processed/          # å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ raw/               # åŸå§‹æ•°æ®
â”œâ”€â”€ test_data/             # æµ‹è¯•æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ processed/          # å¤„ç†åçš„æµ‹è¯•æ•°æ®
â”‚   â””â”€â”€ raw/               # åŸå§‹æµ‹è¯•æ•°æ®
â”œâ”€â”€ docs/                  # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ DISTRIBUTED_TRAINING_GUIDE.md # åˆ†å¸ƒå¼è®­ç»ƒæŒ‡å—
â”‚   â”œâ”€â”€ DEEPSPEED_INTEGRATION.md # DeepSpeedé›†æˆ
â”‚   â”œâ”€â”€ PEFT_USAGE_GUIDE.md # PEFTä½¿ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ LORA_QLORA_GUIDE.md # LoRA/QLoRAæŒ‡å—
â”‚   â”œâ”€â”€ CUSTOM_REWARD_FUNCTIONS.md # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
â”‚   â”œâ”€â”€ multi_datasets_guide.md # å¤šæ•°æ®é›†æŒ‡å—
â”‚   â””â”€â”€ validation_datasets_guide.md # éªŒè¯æ•°æ®é›†æŒ‡å—
â””â”€â”€ src/                    # æ ¸å¿ƒæºä»£ç 
    â”œâ”€â”€ algorithms/          # å¼ºåŒ–å­¦ä¹ ç®—æ³•
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ base.py         # åŸºç¡€ç®—æ³•ç±»
    â”‚   â”œâ”€â”€ ppo.py          # PPOç®—æ³•
    â”‚   â”œâ”€â”€ dpo.py          # DPOç®—æ³•
    â”‚   â”œâ”€â”€ grpo.py         # GRPOç®—æ³•
    â”‚   â””â”€â”€ utils.py        # ç®—æ³•å·¥å…·å‡½æ•°
    â”œâ”€â”€ data/               # æ•°æ®å¤„ç†æ¨¡å—
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ processor.py    # æ•°æ®å¤„ç†å™¨
    â”‚   â”œâ”€â”€ dataset.py      # æ•°æ®é›†ç±»
    â”‚   â”œâ”€â”€ collator.py     # æ•°æ®æ•´ç†å™¨
    â”‚   â”œâ”€â”€ merger.py       # æ•°æ®åˆå¹¶å™¨
    â”‚   â””â”€â”€ utils.py        # æ•°æ®å·¥å…·å‡½æ•°
    â”œâ”€â”€ evaluators/         # è¯„ä¼°æ¨¡å—
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ model_evaluator.py    # æ¨¡å‹è¯„ä¼°å™¨
    â”‚   â”œâ”€â”€ automatic_evaluator.py # è‡ªåŠ¨è¯„ä¼°å™¨
    â”‚   â”œâ”€â”€ human_evaluator.py    # äººå·¥è¯„ä¼°å™¨
    â”‚   â””â”€â”€ metrics.py      # è¯„ä¼°æŒ‡æ ‡
    â”œâ”€â”€ models/             # æ¨¡å‹å®šä¹‰
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ policy_model.py # ç­–ç•¥æ¨¡å‹
    â”‚   â”œâ”€â”€ value_model.py  # ä»·å€¼æ¨¡å‹
    â”‚   â”œâ”€â”€ reward_model.py # å¥–åŠ±æ¨¡å‹
    â”‚   â””â”€â”€ model_utils.py  # æ¨¡å‹å·¥å…·å‡½æ•°
    â”œâ”€â”€ rewards/            # å¥–åŠ±å‡½æ•°æ¨¡å—
    â”‚   â””â”€â”€ custom_rewards.py # è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
    â”œâ”€â”€ trainers/           # è®­ç»ƒå™¨
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ base_trainer.py # åŸºç¡€è®­ç»ƒå™¨
    â”‚   â”œâ”€â”€ sft_trainer.py  # SFTè®­ç»ƒå™¨
    â”‚   â”œâ”€â”€ reward_trainer.py # å¥–åŠ±æ¨¡å‹è®­ç»ƒå™¨
    â”‚   â”œâ”€â”€ ppo_trainer.py  # PPOè®­ç»ƒå™¨
    â”‚   â”œâ”€â”€ dpo_trainer.py  # DPOè®­ç»ƒå™¨
    â”‚   â”œâ”€â”€ grpo_trainer.py # GRPOè®­ç»ƒå™¨
    â”‚   â””â”€â”€ trainer_utils.py # è®­ç»ƒå·¥å…·å‡½æ•°
    â””â”€â”€ utils/              # å·¥å…·æ¨¡å—
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ logger.py       # æ—¥å¿—è®°å½•
        â”œâ”€â”€ config.py       # é…ç½®ç®¡ç†
        â”œâ”€â”€ metrics.py      # è¯„ä¼°æŒ‡æ ‡
        â”œâ”€â”€ checkpoint.py   # æ£€æŸ¥ç‚¹ç®¡ç†
        â”œâ”€â”€ distributed.py  # åˆ†å¸ƒå¼å·¥å…·
        â””â”€â”€ deepspeed_utils.py # DeepSpeedå·¥å…·
```

## ğŸ› ï¸ å®‰è£…

### ç¯å¢ƒè¦æ±‚

- Python >= 3.8
- PyTorch >= 1.12.0
- CUDA >= 11.0 (å¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿ)

### å®‰è£…æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**
```bash
git clone <repository-url>
cd rl_learning
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **å®‰è£…ä¾èµ–**
```bash
# åŸºç¡€å®‰è£…
pip install -r requirements.txt

# éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸ
python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
python -c "import transformers; print(f'Transformersç‰ˆæœ¬: {transformers.__version__}')"

# ç¯å¢ƒæ£€æŸ¥è„šæœ¬
python scripts/check_environment.py
```

### ğŸ”§ é…ç½®æ–‡ä»¶éªŒè¯

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œå»ºè®®éªŒè¯é…ç½®æ–‡ä»¶çš„æ­£ç¡®æ€§ï¼š

```python
# scripts/validate_config.py
import yaml
import os
from pathlib import Path

def validate_config(config_path="config.yaml"):
    """éªŒè¯é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # æ£€æŸ¥å¿…éœ€çš„é…ç½®é¡¹
        required_keys = ['model', 'training', 'data']
        for key in required_keys:
            if key not in config:
                print(f"âŒ ç¼ºå°‘å¿…éœ€é…ç½®é¡¹: {key}")
                return False
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if 'model_name_or_path' in config['model']:
            model_path = config['model']['model_name_or_path']
            if not os.path.exists(model_path) and '/' not in model_path:
                print(f"âš ï¸  æ¨¡å‹è·¯å¾„å¯èƒ½ä¸å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥æ•°æ®è·¯å¾„
        if 'datasets' in config['data']:
            for dataset in config['data']['datasets']:
                if 'path' in dataset:
                    data_path = dataset['path']
                    if not os.path.exists(data_path):
                        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                        return False
        
        print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    validate_config()
```

```bash
# éªŒè¯é…ç½®æ–‡ä»¶
python scripts/validate_config.py
```

## âš™ï¸ é…ç½®

### ä¸»é…ç½®æ–‡ä»¶ (config.yaml)

```yaml
# RL Learning Framework Configuration

# æ¨¡å‹é…ç½®
model:
  name_or_path: "Qwen/Qwen2.5-3B-Instruct"           # åŸºç¡€æ¨¡å‹è·¯å¾„
  cache_dir: "./cache"                       # æ¨¡å‹ç¼“å­˜ç›®å½•

# è®­ç»ƒé…ç½®
training:
  algorithm: "ppo"                          # ç®—æ³•ç±»å‹: ppo, dpo, grpo
  output_dir: "./output"                    # è¾“å‡ºç›®å½•
  num_epochs: 3                             # è®­ç»ƒè½®æ•°
  learning_rate: 5e-5                       # å­¦ä¹ ç‡
  per_device_train_batch_size: 4            # æ¯è®¾å¤‡è®­ç»ƒæ‰¹å¤§å°
  gradient_accumulation_steps: 1            # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
  max_grad_norm: 1.0                        # æ¢¯åº¦è£å‰ª
  warmup_steps: 100                         # é¢„çƒ­æ­¥æ•°
  logging_steps: 10                         # æ—¥å¿—è®°å½•é—´éš”
  save_steps: 500                           # ä¿å­˜é—´éš”
  eval_steps: 500                           # è¯„ä¼°é—´éš”

# PPOç‰¹å®šé…ç½®
ppo:
  ppo_epochs: 4                             # PPOè®­ç»ƒè½®æ•°
  clip_range: 0.2                           # è£å‰ªèŒƒå›´
  value_loss_coef: 0.5                      # ä»·å€¼æŸå¤±ç³»æ•°
  entropy_coef: 0.01                        # ç†µæŸå¤±ç³»æ•°
  gamma: 0.99                               # æŠ˜æ‰£å› å­
  gae_lambda: 0.95                          # GAEå‚æ•°

# DPOç‰¹å®šé…ç½®
dpo:
  beta: 0.1                                 # DPOæ¸©åº¦å‚æ•°
  loss_type: "sigmoid"                      # æŸå¤±ç±»å‹
  label_smoothing: 0.0                      # æ ‡ç­¾å¹³æ»‘

# æ•°æ®é…ç½®
data:
  dataset_path: "./data"                    # æ•°æ®é›†è·¯å¾„
  max_length: 2048                          # æœ€å¤§åºåˆ—é•¿åº¦
  max_prompt_length: 1024                   # æœ€å¤§æç¤ºé•¿åº¦

# ç”Ÿæˆé…ç½®
generation:
  max_new_tokens: 1024                       # æœ€å¤§ç”Ÿæˆé•¿åº¦
  temperature: 1.0                          # é‡‡æ ·æ¸©åº¦
  top_k: 50                                 # Top-ké‡‡æ ·
  top_p: 1.0                                # Top-pé‡‡æ ·

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"                             # æ—¥å¿—çº§åˆ«
  log_dir: "./logs"                         # æ—¥å¿—ç›®å½•
  tensorboard: true                         # å¯ç”¨TensorBoard```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### âš¡ é›¶é…ç½®å¿«é€Ÿä½“éªŒï¼ˆæ¨èé¦–æ¬¡ä½¿ç”¨ï¼‰

æœ€å¿«é€Ÿçš„ä½“éªŒæ–¹å¼ï¼Œæ— éœ€ä»»ä½•é…ç½®ï¼Œ5åˆ†é’Ÿå†…å®Œæˆå®Œæ•´æµç¨‹ï¼š

```bash
# 1. ç¯å¢ƒæ£€æŸ¥å’Œä¾èµ–éªŒè¯
python quick_test.py --check_env

# 2. é›¶é…ç½®å¿«é€Ÿä½“éªŒï¼ˆè‡ªåŠ¨ç”Ÿæˆæ•°æ®ã€å¿«é€Ÿè®­ç»ƒã€è¯„ä¼°ï¼‰
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 10 --quick_test

# 3. æŸ¥çœ‹è®­ç»ƒç»“æœ
python main.py --config config.yaml --mode eval --quick_eval
```

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ç¤ºä¾‹è„šæœ¬ï¼ˆæ¨èæ–°æ‰‹ï¼‰

`example_training.py` æä¾›äº†å®Œæ•´çš„ç«¯åˆ°ç«¯è®­ç»ƒç¤ºä¾‹ï¼ŒåŒ…å«æ•°æ®å‡†å¤‡ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ï¼š

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆä½¿ç”¨å°æ¨¡å‹å’Œå°‘é‡æ­¥æ•°ï¼‰
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 50

# ä½¿ç”¨Qwen/Qwen2.5-3B-Instructè¿›è¡Œå®Œæ•´è®­ç»ƒ
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 1000

# ä»…è¿è¡ŒSFTè®­ç»ƒ
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 100 --skip_reward --skip_rl

# è·³è¿‡è®­ç»ƒï¼Œä»…è¿›è¡Œè¯„ä¼°
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --skip_training
```

**example_training.py ç‰¹ç‚¹**ï¼š
- âœ… è‡ªåŠ¨ç”Ÿæˆç¤ºä¾‹æ•°æ®
- âœ… å®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹ï¼ˆSFT â†’ Reward Model â†’ PPO/DPO/GRPOï¼‰
- âœ… å†…ç½®æ¨¡å‹è¯„ä¼°å’Œæ ·æœ¬ç”Ÿæˆ
- âœ… é€‚åˆå­¦ä¹ å’Œå¿«é€Ÿæµ‹è¯•

### æ–¹å¼äºŒï¼šä½¿ç”¨ç”Ÿäº§çº§æ¡†æ¶

`main.py` æ˜¯ç”Ÿäº§çº§è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒé…ç½®æ–‡ä»¶é©±åŠ¨å’Œé«˜çº§åŠŸèƒ½ï¼š

```bash
# æ•°æ®å¤„ç†
python main.py --config config.yaml --mode data_process

# 1. SFTè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm sft

# 2. å¥–åŠ±æ¨¡å‹è®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm reward

# 3. å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ˆPPO/DPO/GRPOï¼‰
python main.py --config config.yaml --mode train --algorithm ppo
python main.py --config config.yaml --mode train --algorithm dpo
python main.py --config config.yaml --mode train --algorithm grpo

# è¯„ä¼°æ¨¡å‹
python main.py --config config.yaml --mode eval
```

**main.py ç‰¹ç‚¹**ï¼š
- âœ… é…ç½®æ–‡ä»¶é©±åŠ¨ï¼Œçµæ´»æ€§é«˜
- âœ… æ”¯æŒæ–­ç‚¹ç»­è®­
- âœ… ç”Ÿäº§çº§é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- âœ… é€‚åˆæ­£å¼é¡¹ç›®å’Œå¤§è§„æ¨¡è®­ç»ƒ

#### å¤šæ•°æ®é›†é…ç½® ğŸ†•

ç°åœ¨æ”¯æŒé…ç½®å¤šä¸ªæ•°æ®é›†è¿›è¡Œåˆå¹¶è®­ç»ƒï¼åœ¨ `config.yaml` ä¸­é…ç½®ï¼š

```yaml
data:
  datasets:
    sft:
      train_files:
        - path: "./data/alpaca_chinese_sft.json"
          weight: 1.0
          name: "alpaca_chinese"
        - path: "./data/belle_sft.json"
          weight: 0.8
          name: "belle_conversation"
      validation_files:  # æ–°å¢ï¼šéªŒè¯æ•°æ®é›†æ”¯æŒ
        - path: "./data/alpaca_chinese_val.json"
          weight: 1.0
          name: "alpaca_chinese_val"
        - path: "./data/belle_val.json"
          weight: 0.8
          name: "belle_val"
      merge_datasets: true
      merged_cache_path: "./cache/merged_sft_train.json"
      merged_validation_cache_path: "./cache/merged_sft_validation.json"  # æ–°å¢ï¼šéªŒè¯é›†ç¼“å­˜
  merge_config:
    strategy: "weighted_sampling"  # concat, weighted_sampling, balanced
    shuffle: true
```

è¯¦ç»†é…ç½®è¯·å‚è€ƒ [å¤šæ•°æ®é›†é…ç½®æŒ‡å—](docs/multi_datasets_guide.md)

#### æ•°æ®é›†åˆå¹¶ï¼ˆå¯é€‰ï¼‰

æ‰‹åŠ¨åˆå¹¶å¤šä¸ªæ•°æ®é›†ï¼š

```bash
# åˆå¹¶SFTæ•°æ®é›†
python merge_datasets.py --config config.yaml --algorithm sft

# æŸ¥çœ‹åˆå¹¶è®¡åˆ’ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰
python merge_datasets.py --config config.yaml --algorithm sft --dry-run

# ä½¿ç”¨ç‰¹å®šç­–ç•¥åˆå¹¶
python merge_datasets.py --config config.yaml --algorithm sft --strategy balanced
```

### å¿«é€Ÿæµ‹è¯•ç³»ç»Ÿ

```bash
# è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ŒéªŒè¯ç¯å¢ƒé…ç½®
python quick_test.py
```

## ğŸ“Š æ•°æ®å‡†å¤‡

### ğŸ“‹ æ”¯æŒçš„æ•°æ®é›†

#### 1. å†…ç½®æ•°æ®é›†

**Sample (ç¤ºä¾‹æ•°æ®)**
- **æè¿°**: å†…ç½®çš„ä¸­æ–‡æŠ€æœ¯é—®ç­”æ ·æœ¬æ•°æ®ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•
- **æ•°æ®é‡**: å¯é…ç½®ï¼ˆé»˜è®¤100ä¸ªSFTæ ·æœ¬ï¼Œ50ä¸ªåå¥½æ ·æœ¬ï¼Œ20ä¸ªè¯„ä¼°æ ·æœ¬ï¼‰
- **æ ¼å¼**: åŒ…å«SFTã€åå¥½å­¦ä¹ ã€è¯„ä¼°ä¸‰ç§æ ¼å¼
- **ç”¨é€”**: å¿«é€Ÿå¼€å§‹ã€ç³»ç»Ÿæµ‹è¯•ã€ç®—æ³•éªŒè¯
- **è¯­è¨€**: ä¸­æ–‡

#### 2. è‹±æ–‡æ•°æ®é›†

**HH-RLHF (Anthropic Human Feedback)**
- **æè¿°**: Anthropicå‘å¸ƒçš„äººç±»åå¥½æ•°æ®é›†ï¼ŒåŒ…å«æœ‰ç”¨æ€§å’Œæ— å®³æ€§ä¸¤ä¸ªç»´åº¦
- **æ•°æ®é‡**: ~161,000æ¡å¯¹è¯
- **æ ¼å¼**: åå¥½å¯¹æ¯”æ•°æ® (chosen vs rejected)
- **ç”¨é€”**: å¥–åŠ±æ¨¡å‹è®­ç»ƒã€DPOè®­ç»ƒã€GRPOè®­ç»ƒ
- **è¯­è¨€**: è‹±æ–‡

#### 3. ä¸­æ–‡æ•°æ®é›†

**BELLE**
- **æè¿°**: ä¸­æ–‡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†
- **æ•°æ®é‡**: ~2,000,000æ¡æŒ‡ä»¤-å›ç­”å¯¹
- **æ ¼å¼**: æŒ‡ä»¤è·Ÿéšæ•°æ®
- **ç”¨é€”**: SFTè®­ç»ƒ
- **è¯­è¨€**: ä¸­æ–‡

**Alpaca Chinese**
- **æè¿°**: Alpacaæ•°æ®é›†çš„ä¸­æ–‡ç‰ˆæœ¬
- **æ•°æ®é‡**: ~52,000æ¡æŒ‡ä»¤-å›ç­”å¯¹
- **æ ¼å¼**: æŒ‡ä»¤è·Ÿéšæ•°æ®
- **ç”¨é€”**: SFTè®­ç»ƒ
- **è¯­è¨€**: ä¸­æ–‡

**MOSS**
- **æè¿°**: ä¸­æ–‡å¯¹è¯æ•°æ®é›†
- **æ•°æ®é‡**: å˜é‡ï¼ˆæ ¹æ®é…ç½®ï¼‰
- **æ ¼å¼**: å¯¹è¯æ•°æ®
- **ç”¨é€”**: SFTè®­ç»ƒã€å¯¹è¯èƒ½åŠ›æå‡
- **è¯­è¨€**: ä¸­æ–‡

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### å®‰è£…ä¾èµ–

```bash
pip install datasets transformers pandas tqdm
```

#### å¿«é€Ÿå¼€å§‹ï¼ˆæ¨èï¼‰

```bash
# ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆæœ€å¿«æ–¹å¼ï¼Œé€‚åˆæµ‹è¯•ï¼‰
python prepare_datasets.py --dataset sample --num_samples 100 --output_dir ./data

# éªŒè¯æ•°æ®æ ¼å¼
python prepare_datasets.py --validate_only

# æ–°å¢ï¼šæ•°æ®é›†æ‹†åˆ†åŠŸèƒ½
# å°†å•ä¸ªæ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
python prepare_datasets.py --split_dataset ./data/large_dataset.json --train_ratio 0.8 --shuffle --random_seed 42

# æ ¹æ®é…ç½®æ–‡ä»¶æ‰¹é‡æ‹†åˆ†æ•°æ®é›†
python prepare_datasets.py --split_config config.yaml --algorithm_type sft
```

#### ä¸‹è½½å…¬å¼€æ•°æ®é›†

```bash
# ä¸‹è½½æ‰€æœ‰å…¬å¼€æ•°æ®é›†ï¼ˆæ¨èï¼‰
python prepare_datasets.py --dataset all --num_samples 5000 --output_dir ./data

# æˆ–è€…å•ç‹¬ä¸‹è½½ç‰¹å®šæ•°æ®é›†
# ä¸‹è½½ä¸­æ–‡æ•°æ®é›†
python prepare_datasets.py --dataset belle --num_samples 10000 --output_dir ./data
python prepare_datasets.py --dataset alpaca-chinese --num_samples 5000 --output_dir ./data
python prepare_datasets.py --dataset moss --num_samples 3000 --output_dir ./data

# ä¸‹è½½è‹±æ–‡åå¥½æ•°æ®
python prepare_datasets.py --dataset hh-rlhf --num_samples 5000 --output_dir ./data
```

**æ”¯æŒçš„æ•°æ®é›†é€‰é¡¹**ï¼š
- **sample**: å†…ç½®æ ·æœ¬æ•°æ®ï¼ˆä¸­æ–‡æŠ€æœ¯é—®ç­”ï¼‰
- **all**: ä¸‹è½½æ‰€æœ‰å…¬å¼€æ•°æ®é›†ï¼ˆHH-RLHF + BELLE + Alpaca Chinese + MOSSï¼‰
- **hh-rlhf**: Anthropicçš„è‹±æ–‡åå¥½æ•°æ®é›†ï¼ˆ16ä¸‡æ¡ï¼‰
- **belle**: ä¸­æ–‡æŒ‡ä»¤æ•°æ®é›†ï¼ˆ200ä¸‡æ¡ï¼‰
- **alpaca-chinese**: ä¸­æ–‡ç‰ˆAlpacaæ•°æ®é›†ï¼ˆ5ä¸‡æ¡ï¼‰
- **moss**: ä¸­æ–‡å¯¹è¯æ•°æ®é›†

### ğŸ“Š æ•°æ®é›†è¯¦ç»†è¯´æ˜

#### HH-RLHFæ•°æ®é›†

**æ•°æ®æ ¼å¼ç¤ºä¾‹**:
```json
{
  "prompt": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
  "chosen": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚é€šè¿‡åˆ†æå¤§é‡æ•°æ®ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥è¯†åˆ«æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹ã€‚",
  "rejected": "æœºå™¨å­¦ä¹ å°±æ˜¯è®©æœºå™¨å­¦ä¹ ã€‚"
}
```

**ä½¿ç”¨åœºæ™¯**:
- DPO (Direct Preference Optimization) è®­ç»ƒ
- å¥–åŠ±æ¨¡å‹è®­ç»ƒ
- PPOè®­ç»ƒä¸­çš„åå¥½è¯„ä¼°

#### BELLEæ•°æ®é›†

**æ•°æ®æ ¼å¼ç¤ºä¾‹**:
```json
{
  "prompt": "è¯·ä»‹ç»ä¸€ä¸‹ä¸­å›½çš„ä¼ ç»ŸèŠ‚æ—¥æ˜¥èŠ‚ã€‚",
  "response": "æ˜¥èŠ‚æ˜¯ä¸­å›½æœ€é‡è¦çš„ä¼ ç»ŸèŠ‚æ—¥ï¼Œä¹Ÿè¢«ç§°ä¸ºå†œå†æ–°å¹´ã€‚å®ƒæ ‡å¿—ç€å†œå†å¹´çš„å¼€å§‹ï¼Œé€šå¸¸åœ¨1æœˆæˆ–2æœˆä¸¾è¡Œã€‚æ˜¥èŠ‚æœŸé—´ï¼Œäººä»¬ä¼šè¿›è¡Œå„ç§åº†ç¥æ´»åŠ¨ï¼Œå¦‚è´´æ˜¥è”ã€æ”¾é­ç‚®ã€åƒå›¢åœ†é¥­ã€ç»™çº¢åŒ…ç­‰ã€‚è¿™ä¸ªèŠ‚æ—¥è±¡å¾ç€æ–°çš„å¼€å§‹å’Œå®¶åº­å›¢èšã€‚"
}
```

**ä½¿ç”¨åœºæ™¯**:
- ç›‘ç£å¾®è°ƒ (SFT)
- ç”Ÿæˆåˆæˆåå¥½æ•°æ®
- ä¸­æ–‡å¯¹è¯èƒ½åŠ›è®­ç»ƒ

#### Alpaca Chineseæ•°æ®é›†

**æ•°æ®æ ¼å¼ç¤ºä¾‹**:
```json
{
  "prompt": "è§£é‡Šä»¥ä¸‹æ¦‚å¿µï¼šæ·±åº¦å­¦ä¹ \n\næ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ",
  "response": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±å±‚ç¥ç»ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºï¼Œä»ç®€å•çš„ç‰¹å¾åˆ°å¤æ‚çš„æ¦‚å¿µã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚"
}
```

**ä½¿ç”¨åœºæ™¯**:
- ç›‘ç£å¾®è°ƒ (SFT)
- ä¸­æ–‡æŒ‡ä»¤è·Ÿéšèƒ½åŠ›è®­ç»ƒ
- åŸºç¡€å¯¹è¯èƒ½åŠ›å»ºç«‹

### ğŸ”§ æ•°æ®å¤„ç†æµç¨‹

#### 1. æ•°æ®ä¸‹è½½å’Œé¢„å¤„ç†

```python
from prepare_datasets import DatasetPreparer

# åˆ›å»ºæ•°æ®å‡†å¤‡å™¨
preparer = DatasetPreparer(
    output_dir="./data",
    tokenizer_name="Qwen/Qwen2.5-3B-Instruct"
)

# ä¸‹è½½BELLEæ•°æ®é›†
belle_file = preparer.download_belle_data(max_samples=5000)

# éªŒè¯æ•°æ®è´¨é‡
stats = preparer.validate_data(belle_file, "sft")
print(f"æ•°æ®ç»Ÿè®¡: {stats}")
```

#### 2. åˆ›å»ºåå¥½æ•°æ®

```python
# ä»SFTæ•°æ®åˆ›å»ºåˆæˆåå¥½æ•°æ®
preference_file = preparer.create_synthetic_preference_data(
    sft_file=belle_file,
    num_samples=1000
)
```

#### 3. æ•°æ®éªŒè¯

```python
# éªŒè¯åå¥½æ•°æ®
stats = preparer.validate_data(preference_file, "preference")
print(f"åå¥½æ•°æ®ç»Ÿè®¡: {stats}")
```

### ğŸ“ˆ è®­ç»ƒæ•°æ®é…ç½®

#### SFTè®­ç»ƒé…ç½®

```yaml
# config.yaml (SFTè®­ç»ƒé…ç½®)
model:
  name: "Qwen/Qwen2.5-3B-Instruct"
  model_name_or_path: "Qwen/Qwen2.5-3B-Instruct"
  cache_dir: "./cache"

training:
  algorithm: "sft"
  output_dir: "./output/sft"
  num_epochs: 3
  learning_rate: 5e-5
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2

sft:
  max_length: 2048

data:
  dataset_path: "./data/belle_sft_train.json"
  max_length: 2048
  max_prompt_length: 1024
```

#### PPOè®­ç»ƒé…ç½®

```yaml
# config.yaml (PPOè®­ç»ƒé…ç½®)
model:
  name: "Qwen/Qwen2.5-3B-Instruct"
  model_name_or_path: "./output/sft/final_model"  # ä½¿ç”¨SFTè®­ç»ƒåçš„æ¨¡å‹
  cache_dir: "./cache"

training:
  algorithm: "ppo"
  output_dir: "./output/ppo"
  num_epochs: 2
  learning_rate: 1e-5
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4

ppo:
  ppo_epochs: 4
  clip_range: 0.2
  value_loss_coef: 0.5
  entropy_coef: 0.01
  reward_model_path: "./output/reward/final_model"

data:
  dataset_path: "./data/synthetic_preference_train.json"
  max_length: 2048
```

#### DPOè®­ç»ƒé…ç½®

```yaml
# config.yaml (DPOè®­ç»ƒé…ç½®)
model:
  name: "Qwen/Qwen2.5-3B-Instruct"
  model_name_or_path: "./output/sft/final_model"
  cache_dir: "./cache"

training:
  algorithm: "dpo"
  output_dir: "./output/dpo"
  num_epochs: 1
  learning_rate: 5e-6
  per_device_train_batch_size: 2

dpo:
  beta: 0.1
  loss_type: "sigmoid"
  label_smoothing: 0.0

data:
  dataset_path: "./data/hh_rlhf_helpful-base_train.json"
  max_length: 2048
```

### ğŸ¯ è®­ç»ƒæµç¨‹å»ºè®®

#### é˜¶æ®µ1: ç›‘ç£å¾®è°ƒ (SFT)

```bash
# 1. å‡†å¤‡æ‰€æœ‰æ•°æ®ï¼ˆæ¨èï¼‰
python prepare_datasets.py --dataset all --num_samples 10000 --output_dir ./data

# æˆ–è€…å•ç‹¬å‡†å¤‡SFTæ•°æ®
# python prepare_datasets.py --dataset belle --num_samples 10000 --output_dir ./data
# python prepare_datasets.py --dataset alpaca-chinese --num_samples 5000 --output_dir ./data

# 2. å¼€å§‹SFTè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm sft
```

**ç›®æ ‡**: è®©æ¨¡å‹å­¦ä¼šåŸºæœ¬çš„å¯¹è¯å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›

#### é˜¶æ®µ2: å¥–åŠ±æ¨¡å‹è®­ç»ƒ

```bash
# 1. å‡†å¤‡åå¥½æ•°æ®ï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰ä½¿ç”¨ --dataset allï¼‰
python prepare_datasets.py --dataset hh-rlhf --num_samples 5000 --output_dir ./data

# 2. è®­ç»ƒå¥–åŠ±æ¨¡å‹
python main.py --config config.yaml --mode train --algorithm reward
```

**ç›®æ ‡**: è®­ç»ƒå¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°å›ç­”è´¨é‡

#### é˜¶æ®µ3: åå¥½ä¼˜åŒ– (PPO/DPO/GRPO)

**é€‰æ‹©PPO**:
```bash
# PPOè®­ç»ƒï¼ˆéœ€è¦å¥–åŠ±æ¨¡å‹ï¼‰
python main.py --config config.yaml --mode train --algorithm ppo
```

**é€‰æ‹©DPO**:
```bash
# DPOè®­ç»ƒï¼ˆæ— éœ€å¥–åŠ±æ¨¡å‹ï¼‰
python main.py --config config.yaml --mode train --algorithm dpo
```

**é€‰æ‹©GRPO**:
```bash
# GRPOè®­ç»ƒï¼ˆéœ€è¦å¥–åŠ±æ¨¡å‹ï¼‰
python main.py --config config.yaml --mode train --algorithm grpo
```

**ç›®æ ‡**: è®©æ¨¡å‹å­¦ä¼šäººç±»åå¥½ï¼Œæé«˜å›ç­”è´¨é‡

### ğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°

#### è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

```python
from src.utils import MetricsTracker

# è¯„ä¼°æ•°æ®è´¨é‡
metrics = {
    "avg_prompt_length": 45.2,
    "avg_response_length": 128.7,
    "empty_samples": 0,
    "total_samples": 5000
}

print(f"æ•°æ®è´¨é‡æŠ¥å‘Š:")
print(f"- å¹³å‡æç¤ºé•¿åº¦: {metrics['avg_prompt_length']:.1f} å­—ç¬¦")
print(f"- å¹³å‡å›ç­”é•¿åº¦: {metrics['avg_response_length']:.1f} å­—ç¬¦")
print(f"- ç©ºæ ·æœ¬æ•°é‡: {metrics['empty_samples']}")
print(f"- æ€»æ ·æœ¬æ•°é‡: {metrics['total_samples']}")
```

#### æ‰‹åŠ¨è´¨é‡æ£€æŸ¥

```python
import json
import random

# éšæœºæ£€æŸ¥æ•°æ®æ ·æœ¬
with open('./data/belle_sft_train.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# éšæœºé€‰æ‹©5ä¸ªæ ·æœ¬è¿›è¡Œæ£€æŸ¥
samples = random.sample(data, 5)
for i, sample in enumerate(samples):
    print(f"\n=== æ ·æœ¬ {i+1} ===")
    print(f"æç¤º: {sample['prompt']}")
    print(f"å›ç­”: {sample['response']}")
    print("-" * 50)
```

### ğŸ“š è¿›é˜¶ç”¨æ³•

#### è‡ªå®šä¹‰æ•°æ®é›†

```python
# åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†
custom_data = [
    {
        "prompt": "ä½ çš„è‡ªå®šä¹‰é—®é¢˜",
        "response": "æœŸæœ›çš„å›ç­”"
    },
    # æ›´å¤šæ•°æ®...
]

# ä¿å­˜ä¸ºJSONæ ¼å¼
with open('./data/custom_sft_train.json', 'w', encoding='utf-8') as f:
    json.dump(custom_data, f, ensure_ascii=False, indent=2)
```

#### æ•°æ®æ··åˆç­–ç•¥

```python
# æ··åˆå¤šä¸ªæ•°æ®é›†
def merge_datasets(file_paths, output_path, weights=None):
    all_data = []
    
    for i, file_path in enumerate(file_paths):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # åº”ç”¨æƒé‡
        if weights:
            data = data[:int(len(data) * weights[i])]
        
        all_data.extend(data)
    
    # éšæœºæ‰“ä¹±
    random.shuffle(all_data)
    
    # ä¿å­˜æ··åˆæ•°æ®
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"æ··åˆæ•°æ®é›†å·²ä¿å­˜: {output_path}")
    print(f"æ€»æ ·æœ¬æ•°: {len(all_data)}")

# ä½¿ç”¨ç¤ºä¾‹
merge_datasets(
    ['./data/belle_sft_train.json', './data/alpaca_chinese_sft_train.json'],
    './data/mixed_sft_train.json',
    weights=[0.7, 0.3]  # BELLEå 70%ï¼ŒAlpacaå 30%
)
```

#### æ•°æ®å¢å¼º

```python
# ç®€å•çš„æ•°æ®å¢å¼ºç­–ç•¥
def augment_data(input_file, output_file, augment_ratio=0.2):
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    augmented_data = data.copy()
    
    # éšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œå¢å¼º
    samples_to_augment = random.sample(data, int(len(data) * augment_ratio))
    
    for sample in samples_to_augment:
        # ç®€å•çš„å¢å¼ºï¼šåœ¨promptå‰æ·»åŠ ç¤¼è²Œç”¨è¯­
        augmented_sample = sample.copy()
        augmented_sample['prompt'] = f"è¯·é—®ï¼Œ{sample['prompt']}"
        augmented_data.append(augmented_sample)
    
    # ä¿å­˜å¢å¼ºåçš„æ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(augmented_data, f, ensure_ascii=False, indent=2)
    
    print(f"æ•°æ®å¢å¼ºå®Œæˆ: {len(data)} -> {len(augmented_data)} æ ·æœ¬")

# ä½¿ç”¨ç¤ºä¾‹
augment_data('./data/belle_sft_train.json', './data/belle_sft_augmented.json')
```

### ğŸ” æ•°æ®ç›¸å…³æ•…éšœæ’é™¤

#### å¸¸è§é—®é¢˜

1. **ä¸‹è½½å¤±è´¥**
   ```bash
   # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
   export HF_ENDPOINT=https://hf-mirror.com
   python prepare_datasets.py --dataset belle --num_samples 10000 --output_dir ./data
   ```

2. **å†…å­˜ä¸è¶³**
   ```bash
   # å‡å°‘æ ·æœ¬æ•°é‡
   python prepare_datasets.py --dataset sample --num_samples 1000 --output_dir ./data
   ```

3. **åˆ†è¯å™¨åŠ è½½å¤±è´¥**
   ```bash
   # ä½¿ç”¨æœ¬åœ°åˆ†è¯å™¨
   python prepare_datasets.py --tokenizer ./local_tokenizer
   ```

#### æ•°æ®æ ¼å¼éªŒè¯

```python
# éªŒè¯æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
def validate_format(file_path, expected_format):
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if expected_format == "sft":
        required_keys = ["prompt", "response"]
    elif expected_format == "preference":
        required_keys = ["prompt", "chosen", "rejected"]
    
    for i, item in enumerate(data[:5]):  # æ£€æŸ¥å‰5ä¸ªæ ·æœ¬
        for key in required_keys:
            if key not in item:
                print(f"é”™è¯¯: æ ·æœ¬ {i} ç¼ºå°‘å­—æ®µ '{key}'")
                return False
    
    print(f"æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡: {expected_format}")
    return True

# ä½¿ç”¨ç¤ºä¾‹
validate_format('./data/belle_sft_train.json', 'sft')
validate_format('./data/hh_rlhf_helpful-base_train.json', 'preference')
```

### æ•°æ®æ ¼å¼

æ¡†æ¶æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼š

#### 1. SFT (Supervised Fine-tuning) æ•°æ®
```json
{
  "prompt": "ç”¨æˆ·é—®é¢˜æˆ–æŒ‡ä»¤",
  "response": "æœŸæœ›çš„å›ç­”"
}
```

#### 2. åå¥½å­¦ä¹ æ•°æ®
```json
{
  "prompt": "ç”¨æˆ·é—®é¢˜",
  "chosen": "æ›´å¥½çš„å›ç­”",
  "rejected": "è¾ƒå·®çš„å›ç­”"
}
```

#### 3. å¯¹è¯æ•°æ®
```json
{
  "conversations": [
    {"role": "user", "content": "ç”¨æˆ·æ¶ˆæ¯"},
    {"role": "assistant", "content": "åŠ©æ‰‹å›å¤"}
  ]
}
```

### éªŒè¯æ•°æ®é›†åŠŸèƒ½

æ¡†æ¶ç°åœ¨æ”¯æŒç‹¬ç«‹çš„éªŒè¯æ•°æ®é›†é…ç½®ï¼Œæä¾›æ›´å¥½çš„è®­ç»ƒç›‘æ§å’Œæ¨¡å‹è¯„ä¼°ï¼š

#### åŠŸèƒ½ç‰¹æ€§

- **å¤šéªŒè¯æ–‡ä»¶æ”¯æŒ**: å¯é…ç½®å¤šä¸ªéªŒè¯æ•°æ®é›†æ–‡ä»¶
- **è‡ªåŠ¨åˆå¹¶**: éªŒè¯æ•°æ®é›†å¯è‡ªåŠ¨åˆå¹¶å¹¶ç¼“å­˜
- **ç®—æ³•å…¼å®¹**: æ”¯æŒSFTã€Rewardå’ŒRLHFæ‰€æœ‰è®­ç»ƒç®—æ³•
- **æ•°æ®é›†æ‹†åˆ†**: æä¾›å·¥å…·å°†å¤§æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

#### é…ç½®ç¤ºä¾‹

```yaml
# config.yaml
data:
  sft:
    train_files:
      - "./data/sft_train1.json"
      - "./data/sft_train2.json"
    validation_files:  # éªŒè¯æ•°æ®é›†é…ç½®
      - "./data/sft_val1.json"
      - "./data/sft_val2.json"
    merge_datasets: true
    merged_cache_path: "./data/merged_sft_train.json"
    merged_validation_cache_path: "./data/merged_sft_validation.json"
```

#### ä½¿ç”¨æ•°æ®é›†æ‹†åˆ†å·¥å…·

```bash
# å°†å•ä¸ªæ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
python prepare_datasets.py --split_dataset ./data/large_dataset.json \
    --train_ratio 0.8 --shuffle --random_seed 42

# æ ¹æ®é…ç½®æ–‡ä»¶æ‰¹é‡æ‹†åˆ†
python prepare_datasets.py --split_config config.yaml --algorithm_type sft
```

è¯¦ç»†ä½¿ç”¨æŒ‡å—è¯·å‚è€ƒ `docs/validation_datasets_guide.md`ã€‚

### æ•°æ®é¢„å¤„ç†

```python
from src.data import DataProcessor

# åˆ›å»ºæ•°æ®å¤„ç†å™¨
processor = DataProcessor(
    tokenizer_name="Qwen/Qwen2.5-3B-Instruct",
    max_length=2048
)

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
train_dataset = processor.load_dataset(
    "./data/belle_sft_train.json",
    dataset_type="sft"
)

eval_dataset = processor.load_dataset(
    "./data/hh_rlhf_helpful-base_train.json",
    dataset_type="preference"
)
```

## ğŸ‹ï¸ è®­ç»ƒ

### ğŸ” è®­ç»ƒå‰ç½®æ£€æŸ¥

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œå»ºè®®è¿è¡Œä»¥ä¸‹æ£€æŸ¥è„šæœ¬ï¼š

```python
# scripts/pre_training_check.py
import torch
import psutil
import os
from transformers import AutoTokenizer, AutoModel

def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    print("=== ç³»ç»Ÿèµ„æºæ£€æŸ¥ ===")
    
    # GPUæ£€æŸ¥
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    # å†…å­˜æ£€æŸ¥
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜: {memory.total / 1024**3:.1f}GB (å¯ç”¨: {memory.available / 1024**3:.1f}GB)")
    
    # ç£ç›˜ç©ºé—´æ£€æŸ¥
    disk = psutil.disk_usage('.')
    print(f"ğŸ’¿ ç£ç›˜ç©ºé—´: {disk.free / 1024**3:.1f}GB å¯ç”¨")
    
    if disk.free / 1024**3 < 10:
        print("âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå»ºè®®è‡³å°‘ä¿ç•™10GBç©ºé—´")

def check_model_accessibility(model_name):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯è®¿é—®"""
    print(f"\n=== æ¨¡å‹è®¿é—®æ£€æŸ¥: {model_name} ===")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # å°è¯•åŠ è½½æ¨¡å‹é…ç½®ï¼ˆä¸åŠ è½½æƒé‡ï¼‰
        from transformers import AutoConfig
        config = AutoConfig.from_pretrained(model_name)
        print(f"âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹ç±»å‹: {config.model_type}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
        
        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹è®¿é—®å¤±è´¥: {e}")
        return False

def check_data_files(config_path="config.yaml"):
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    print(f"\n=== æ•°æ®æ–‡ä»¶æ£€æŸ¥ ===")
    try:
        import yaml
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if 'datasets' in config.get('data', {}):
            for i, dataset in enumerate(config['data']['datasets']):
                if 'path' in dataset:
                    path = dataset['path']
                    if os.path.exists(path):
                        size = os.path.getsize(path) / 1024**2
                        print(f"âœ… æ•°æ®é›† {i+1}: {path} ({size:.1f}MB)")
                    else:
                        print(f"âŒ æ•°æ®é›† {i+1}: {path} ä¸å­˜åœ¨")
                        return False
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    import sys
    model_name = sys.argv[1] if len(sys.argv) > 1 else "Qwen/Qwen2.5-3B-Instruct"
    
    check_system_resources()
    check_model_accessibility(model_name)
    check_data_files()
    
    print("\n=== æ£€æŸ¥å®Œæˆ ===")
```

```bash
# è¿è¡Œè®­ç»ƒå‰æ£€æŸ¥
python scripts/pre_training_check.py Qwen/Qwen2.5-3B-Instruct
```

### è®­ç»ƒæµç¨‹æ¦‚è¿°

æœ¬æ¡†æ¶æ”¯æŒå®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹ï¼Œ**å¿…é¡»æŒ‰ç…§ä»¥ä¸‹é¡ºåºè¿›è¡Œè®­ç»ƒ**ï¼š

1. **SFT (Supervised Fine-tuning)** - ç›‘ç£å¾®è°ƒåŸºç¡€æ¨¡å‹
2. **Reward Model** - è®­ç»ƒå¥–åŠ±æ¨¡å‹ç”¨äºè¯„ä¼°å“åº”è´¨é‡  
3. **å¼ºåŒ–å­¦ä¹ è®­ç»ƒ** - ä½¿ç”¨PPOã€DPOæˆ–GRPOç®—æ³•è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹

> âš ï¸ **é‡è¦æç¤º**: 
> - PPOå’ŒGRPOè®­ç»ƒä¾èµ–å¥–åŠ±æ¨¡å‹ï¼Œå¿…é¡»å…ˆå®Œæˆå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼
> - DPOå¯ä»¥ç‹¬ç«‹å·¥ä½œï¼Œä¸éœ€è¦å¥–åŠ±æ¨¡å‹
> - æ¨èä½¿ç”¨ `example_training.py` è¿›è¡Œå®Œæ•´æµç¨‹è®­ç»ƒ

### ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

æ¡†æ¶å…¨é¢æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼Œå¯æ˜¾è‘—åŠ é€Ÿå¤§æ¨¡å‹è®­ç»ƒï¼š

#### æ”¯æŒçš„åˆ†å¸ƒå¼ç­–ç•¥

- **DDP (DistributedDataParallel)**: é€‚ç”¨äºä¸­å°å‹æ¨¡å‹ï¼ˆ<1Bå‚æ•°ï¼‰
- **DeepSpeed ZeRO**: é€‚ç”¨äºå¤§å‹æ¨¡å‹ï¼Œæ”¯æŒå†…å­˜ä¼˜åŒ–
  - ZeRO Stage 1: ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡
  - ZeRO Stage 2: æ¢¯åº¦åˆ†ç‰‡
  - ZeRO Stage 3: å‚æ•°åˆ†ç‰‡ï¼Œæ”¯æŒCPU/NVMeå¸è½½
- **è‡ªåŠ¨ç­–ç•¥é€‰æ‹©**: æ ¹æ®æ¨¡å‹å¤§å°è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥

#### å¿«é€Ÿå¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# å•èŠ‚ç‚¹4GPUè®­ç»ƒï¼ˆè‡ªåŠ¨é€‰æ‹©ç­–ç•¥ï¼‰
python scripts/run_distributed.py \
    --nproc_per_node 4 \
    --config config.yaml \
    --algorithm sft

# ä½¿ç”¨DeepSpeedè®­ç»ƒå¤§æ¨¡å‹
python scripts/run_distributed.py \
    --nproc_per_node 4 \
    --config config_deepspeed_example.yaml \
    --algorithm sft

# å¤šèŠ‚ç‚¹è®­ç»ƒï¼ˆèŠ‚ç‚¹1ï¼‰
python scripts/run_distributed.py \
    --nproc_per_node 4 --nnodes 2 --node_rank 0 \
    --master_addr 192.168.1.100 --master_port 29500 \
    --config config.yaml
```

#### é…ç½®ç¤ºä¾‹

```yaml
# å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
distributed: true
distributed_strategy: "auto"  # è‡ªåŠ¨é€‰æ‹©ï¼šddp, deepspeed, auto
distributed_backend: "nccl"   # GPUä½¿ç”¨ncclï¼ŒCPUä½¿ç”¨gloo

# DeepSpeedé…ç½®ï¼ˆå¯é€‰ï¼‰
deepspeed_config: "configs/deepspeed/zero2.json"
deepspeed_zero_stage: 2
deepspeed_cpu_offload: true
```

> ğŸ“– **è¯¦ç»†æŒ‡å—**: å®Œæ•´çš„åˆ†å¸ƒå¼è®­ç»ƒé…ç½®å’Œä¼˜åŒ–æŒ‡å—è¯·å‚è€ƒ [`docs/DISTRIBUTED_TRAINING_GUIDE.md`](docs/DISTRIBUTED_TRAINING_GUIDE.md)
>
> ğŸš€ **DeepSpeedé›†æˆ**: DeepSpeedåŠŸèƒ½è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ [`DEEPSPEED_INTEGRATION.md`](DEEPSPEED_INTEGRATION.md)

### ä½¿ç”¨ç¤ºä¾‹è„šæœ¬è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
# å®Œæ•´RLHFè®­ç»ƒæµç¨‹
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 1000

# ä»…SFTè®­ç»ƒ
python example_training.py --model_name gpt2 --max_steps 100 --skip_reward --skip_rl

# ä»SFTå¼€å§‹ï¼Œè·³è¿‡å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥DPOè®­ç»ƒ
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 100 --skip_reward --algorithm dpo

# ä½¿ç”¨æ›´å¤§æ¨¡å‹
python example_training.py --model_name Qwen/Qwen2.5-7B --max_steps 500
```

### å‘½ä»¤è¡Œè®­ç»ƒï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

ä½¿ç”¨ `main.py` è¿›è¡Œç”Ÿäº§çº§è®­ç»ƒï¼Œæ”¯æŒå®Œæ•´çš„RLHFæµç¨‹ï¼š

#### 1. SFTè®­ç»ƒ

é¦–å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä¸ºåç»­è®­ç»ƒæä¾›åŸºç¡€æ¨¡å‹ï¼š

```bash
# ä½¿ç”¨main.pyè¿›è¡ŒSFTè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm sft
```

æˆ–è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼š

```python
from src.trainers import SFTTrainer, SFTTrainingConfig
from src.models import create_policy_model
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = create_policy_model("Qwen/Qwen2.5-3B-Instruct")

# é…ç½®è®­ç»ƒå‚æ•°
config = SFTTrainingConfig(
    model_name_or_path="Qwen/Qwen2.5-3B-Instruct",
    output_dir="./output/sft",
    num_epochs=3,
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    max_length=2048
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = SFTTrainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    train_dataset=sft_dataset
)

# å¼€å§‹è®­ç»ƒ
training_result = trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model("./output/sft/final_model")
```

#### 2. Reward Modelè®­ç»ƒ

ä½¿ç”¨åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼š

```bash
# ä½¿ç”¨main.pyè¿›è¡Œå¥–åŠ±æ¨¡å‹è®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm reward
```

æˆ–è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼š

```python
from src.trainers import RewardModelTrainer, RewardTrainingConfig
from src.models import create_reward_model

# åŠ è½½æ¨¡å‹ï¼ˆåŸºäºSFTæ¨¡å‹æˆ–åŸºç¡€æ¨¡å‹ï¼‰
reward_model, tokenizer = create_reward_model("./output/sft/final_model")  # æ¨èä½¿ç”¨SFTæ¨¡å‹

# é…ç½®è®­ç»ƒå‚æ•°
config = RewardTrainingConfig(
    model_name_or_path="./output/sft/final_model",
    output_dir="./output/reward",
    num_epochs=3,
    learning_rate=5e-5,
    margin=0.0,
    loss_type="ranking"
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = RewardModelTrainer(
    config=config,
    model=reward_model,
    tokenizer=tokenizer,
    train_dataset=preference_dataset
)

# å¼€å§‹è®­ç»ƒ
training_result = trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model("./output/reward/final_model")
```

#### 3. å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

å®ŒæˆSFTå’Œå¥–åŠ±æ¨¡å‹è®­ç»ƒåï¼Œå¯ä»¥é€‰æ‹©ä»¥ä¸‹ç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼š

**PPOè®­ç»ƒ**

```bash
# ä½¿ç”¨main.pyè¿›è¡ŒPPOè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm ppo
```

æˆ–è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼š

```python
from src.trainers import PPOTrainer, PPOTrainingConfig
from src.models import create_policy_model, create_value_model, create_reward_model
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
policy_model, _ = create_policy_model("./output/sft/final_model")  # ä½¿ç”¨SFTæ¨¡å‹ä½œä¸ºåˆå§‹ç­–ç•¥æ¨¡å‹
value_model, _ = create_value_model("Qwen/Qwen2.5-3B-Instruct")
reward_model, _ = create_reward_model("./output/reward/final_model")  # åŠ è½½è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹

# é…ç½®è®­ç»ƒå‚æ•°
config = PPOTrainingConfig(
    model_name_or_path="Qwen/Qwen2.5-3B-Instruct",
    output_dir="./output/ppo",
    num_epochs=3,
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    ppo_epochs=4,
    clip_range=0.2,
    value_loss_coef=0.5,
    entropy_coef=0.01,
    reward_model_path="./output/reward/final_model"  # å¥–åŠ±æ¨¡å‹è·¯å¾„
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = PPOTrainer(
    config=config,
    policy_model=policy_model,
    value_model=value_model,
    tokenizer=tokenizer,
    reward_model=reward_model,  # ä¼ å…¥å¥–åŠ±æ¨¡å‹
    train_dataloader=train_dataloader,
    eval_dataloader=eval_dataloader
)

# å¼€å§‹è®­ç»ƒ
training_result = trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model("./output/ppo/final_model")
```

**DPOè®­ç»ƒ**

```bash
# ä½¿ç”¨main.pyè¿›è¡ŒDPOè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm dpo
```

DPOä¸éœ€è¦å¥–åŠ±æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨åå¥½æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ–è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼š

```python
from src.trainers import DPOTrainer, DPOTrainingConfig
from src.models import create_policy_model

# åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨SFTæ¨¡å‹ä½œä¸ºåŸºç¡€ï¼‰
model, tokenizer = create_policy_model("./output/sft/final_model")
reference_model, _ = create_policy_model("./output/sft/final_model")  # å‚è€ƒæ¨¡å‹

# é…ç½®è®­ç»ƒå‚æ•°
config = DPOTrainingConfig(
    model_name_or_path="./output/sft/final_model",
    output_dir="./output/dpo",
    num_epochs=3,
    learning_rate=5e-5,
    beta=0.1,
    loss_type="sigmoid"
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = DPOTrainer(
    config=config,
    model=model,
    tokenizer=tokenizer,
    reference_model=reference_model,
    train_dataloader=train_dataloader,
    eval_dataloader=eval_dataloader
)

# å¼€å§‹è®­ç»ƒ
training_result = trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model("./output/dpo/final_model")
```

**GRPOè®­ç»ƒ**

```bash
# ä½¿ç”¨main.pyè¿›è¡ŒGRPOè®­ç»ƒ
python main.py --config config.yaml --mode train --algorithm grpo
```

GRPOéœ€è¦å¥–åŠ±æ¨¡å‹ï¼Œç¡®ä¿å…ˆå®Œæˆå¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚æˆ–è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ï¼š

```python
from src.trainers import GRPOTrainer, GRPOConfig
from src.models import create_policy_model, create_reward_model

# åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨SFTæ¨¡å‹ä½œä¸ºåŸºç¡€ï¼‰
policy_model, tokenizer = create_policy_model("./output/sft/final_model")
reward_model, _ = create_reward_model("./output/reward/final_model")  # åŠ è½½è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹

# é…ç½®è®­ç»ƒå‚æ•°
config = GRPOConfig(
    model_name_or_path="./output/sft/final_model",
    output_dir="./output/grpo",
    num_epochs=3,
    learning_rate=1e-5,
    group_size=4,
    beta=0.1,
    clip_range=0.2,
    vf_coef=0.5,
    ent_coef=0.01
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = GRPOTrainer(
    config=config,
    policy_model=policy_model,
    reward_model=reward_model,
    tokenizer=tokenizer,
    train_dataloader=train_dataloader,
    eval_dataloader=eval_dataloader
)

# å¼€å§‹è®­ç»ƒ
training_result = trainer.train()

# ä¿å­˜æ¨¡å‹
trainer.save_model("./output/grpo/final_model")
```

### å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹

ä»¥ä¸‹æ˜¯å®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹ç¤ºä¾‹ï¼š

```python
# å®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹
def complete_rlhf_training():
    # 1. SFTè®­ç»ƒ
    print("Step 1: Training SFT model...")
    sft_model_path = train_sft_model("Qwen/Qwen2.5-3B-Instruct")
    
    # 2. å¥–åŠ±æ¨¡å‹è®­ç»ƒ
    print("Step 2: Training Reward model...")
    reward_model_path = train_reward_model(sft_model_path)
    
    # 3. é€‰æ‹©å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒ
    print("Step 3: RL training...")
    
    # é€‰é¡¹A: PPOè®­ç»ƒ
    ppo_model_path = train_ppo_model(sft_model_path, reward_model_path)
    
    # é€‰é¡¹B: DPOè®­ç»ƒï¼ˆä¸éœ€è¦å¥–åŠ±æ¨¡å‹ï¼‰
    # dpo_model_path = train_dpo_model(sft_model_path)
    
    # é€‰é¡¹C: GRPOè®­ç»ƒ
    # grpo_model_path = train_grpo_model(sft_model_path, reward_model_path)
    
    print("Training complete!")

if __name__ == "__main__":
    complete_rlhf_training()
```

### å¿«é€Ÿå¼€å§‹è®­ç»ƒ

ä½¿ç”¨ç¤ºä¾‹è„šæœ¬è¿›è¡Œå®Œæ•´çš„RLHFè®­ç»ƒï¼š

```bash
# å®Œæ•´RLHFè®­ç»ƒæµç¨‹ï¼ˆSFT â†’ Reward Model â†’ PPO/DPO/GRPOï¼‰
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 100

# ä»…è¿›è¡ŒSFTè®­ç»ƒ
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 100 --only_sft

# è·³è¿‡è®­ç»ƒï¼Œä»…è¯„ä¼°ç°æœ‰æ¨¡å‹
python example_training.py --skip_training

# ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
python example_training.py --model_name Qwen/Qwen2.5-3B-Instruct --max_steps 500
```

### å‘½ä»¤è¡Œè®­ç»ƒ

```bash
# PPOè®­ç»ƒï¼ˆéœ€è¦å…ˆå®ŒæˆSFTå’ŒReward Modelè®­ç»ƒï¼‰
python main.py --config config.yaml --algorithm ppo

# DPOè®­ç»ƒï¼ˆéœ€è¦å…ˆå®ŒæˆSFTè®­ç»ƒï¼‰
python main.py --config config.yaml --algorithm dpo

# GRPOè®­ç»ƒï¼ˆéœ€è¦å…ˆå®ŒæˆSFTå’ŒReward Modelè®­ç»ƒï¼‰
python main.py --config config.yaml --algorithm grpo

# æŒ‡å®šGPU
python main.py --config config.yaml --algorithm ppo --device cuda:0

# ä»æ£€æŸ¥ç‚¹æ¢å¤
python main.py --config config.yaml --algorithm ppo --resume_from_checkpoint ./output/ppo/checkpoints/step-1000
```

> âš ï¸ **è®­ç»ƒé¡ºåºæé†’**: 
> - PPOå’ŒGRPOè®­ç»ƒå‰å¿…é¡»å…ˆå®ŒæˆSFTå’ŒReward Modelè®­ç»ƒ
> - DPOè®­ç»ƒå‰åªéœ€å®ŒæˆSFTè®­ç»ƒ
> - å»ºè®®ä½¿ç”¨ `example_training.py` è¿›è¡Œå®Œæ•´æµç¨‹è®­ç»ƒ

## ğŸ§ª æµ‹è¯•å’Œè¯„ä¼°

### ğŸ“Š è®­ç»ƒè¿›åº¦è·Ÿè¸ª

å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦å’Œé¢„ä¼°å®Œæˆæ—¶é—´ï¼š

```python
# scripts/training_monitor.py
import time
import json
from datetime import datetime, timedelta
from pathlib import Path

class TrainingMonitor:
    def __init__(self, total_steps, log_file="training_progress.json"):
        self.total_steps = total_steps
        self.log_file = log_file
        self.start_time = time.time()
        self.step_times = []
        
    def update(self, current_step, loss=None, metrics=None):
        """æ›´æ–°è®­ç»ƒè¿›åº¦"""
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        
        # è®¡ç®—å¹³å‡æ­¥éª¤æ—¶é—´
        if current_step > 0:
            avg_step_time = elapsed_time / current_step
            remaining_steps = self.total_steps - current_step
            eta = remaining_steps * avg_step_time
            
            progress = {
                "current_step": current_step,
                "total_steps": self.total_steps,
                "progress_percent": (current_step / self.total_steps) * 100,
                "elapsed_time": elapsed_time,
                "eta_seconds": eta,
                "eta_formatted": str(timedelta(seconds=int(eta))),
                "avg_step_time": avg_step_time,
                "timestamp": datetime.now().isoformat(),
                "loss": loss,
                "metrics": metrics
            }
            
            # ä¿å­˜è¿›åº¦
            with open(self.log_file, 'w') as f:
                json.dump(progress, f, indent=2)
            
            # æ‰“å°è¿›åº¦
            print(f"\ræ­¥éª¤ {current_step}/{self.total_steps} "
                  f"({progress['progress_percent']:.1f}%) "
                  f"- é¢„è®¡å‰©ä½™: {progress['eta_formatted']} "
                  f"- æŸå¤±: {loss:.4f if loss else 'N/A'}", end="")
            
            return progress
        
    def get_status(self):
        """è·å–å½“å‰çŠ¶æ€"""
        if Path(self.log_file).exists():
            with open(self.log_file, 'r') as f:
                return json.load(f)
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = TrainingMonitor(total_steps=1000)
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    for step in range(1, 101):
        time.sleep(0.1)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
        loss = 2.0 - (step * 0.01)  # æ¨¡æ‹ŸæŸå¤±ä¸‹é™
        monitor.update(step, loss=loss)
```

```bash
# æŸ¥çœ‹è®­ç»ƒè¿›åº¦
python scripts/training_monitor.py

# å®æ—¶ç›‘æ§è®­ç»ƒçŠ¶æ€
watch -n 5 "python -c 'import json; print(json.dumps(json.load(open(\"training_progress.json\")), indent=2))'"
```

### ğŸ”„ æ£€æŸ¥ç‚¹ç®¡ç†å’Œæ¢å¤

```python
# scripts/checkpoint_manager.py
import os
import json
from pathlib import Path
from datetime import datetime

class CheckpointManager:
    def __init__(self, checkpoint_dir="./checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
    def save_checkpoint_info(self, step, loss, model_path, config):
        """ä¿å­˜æ£€æŸ¥ç‚¹ä¿¡æ¯"""
        checkpoint_info = {
            "step": step,
            "loss": loss,
            "model_path": str(model_path),
            "timestamp": datetime.now().isoformat(),
            "config": config
        }
        
        info_file = self.checkpoint_dir / f"checkpoint_{step}_info.json"
        with open(info_file, 'w') as f:
            json.dump(checkpoint_info, f, indent=2)
            
        # æ›´æ–°æœ€æ–°æ£€æŸ¥ç‚¹ä¿¡æ¯
        latest_file = self.checkpoint_dir / "latest_checkpoint.json"
        with open(latest_file, 'w') as f:
            json.dump(checkpoint_info, f, indent=2)
            
        print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {info_file}")
        
    def list_checkpoints(self):
        """åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹"""
        checkpoints = []
        for info_file in self.checkpoint_dir.glob("checkpoint_*_info.json"):
            with open(info_file, 'r') as f:
                info = json.load(f)
                checkpoints.append(info)
        
        # æŒ‰æ­¥éª¤æ’åº
        checkpoints.sort(key=lambda x: x['step'])
        return checkpoints
        
    def get_best_checkpoint(self, metric='loss', mode='min'):
        """è·å–æœ€ä½³æ£€æŸ¥ç‚¹"""
        checkpoints = self.list_checkpoints()
        if not checkpoints:
            return None
            
        if mode == 'min':
            best = min(checkpoints, key=lambda x: x.get(metric, float('inf')))
        else:
            best = max(checkpoints, key=lambda x: x.get(metric, float('-inf')))
            
        return best
        
    def resume_from_checkpoint(self, checkpoint_path=None):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ"""
        if checkpoint_path is None:
            # ä½¿ç”¨æœ€æ–°æ£€æŸ¥ç‚¹
            latest_file = self.checkpoint_dir / "latest_checkpoint.json"
            if latest_file.exists():
                with open(latest_file, 'r') as f:
                    checkpoint_info = json.load(f)
                checkpoint_path = checkpoint_info['model_path']
            else:
                print("âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹")
                return None
                
        if os.path.exists(checkpoint_path):
            print(f"âœ… ä»æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint_path}")
            return checkpoint_path
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    manager = CheckpointManager()
    
    # åˆ—å‡ºæ£€æŸ¥ç‚¹
    checkpoints = manager.list_checkpoints()
    print("å¯ç”¨æ£€æŸ¥ç‚¹:")
    for cp in checkpoints:
        print(f"  æ­¥éª¤ {cp['step']}: æŸå¤± {cp['loss']:.4f} - {cp['timestamp']}")
    
    # è·å–æœ€ä½³æ£€æŸ¥ç‚¹
    best = manager.get_best_checkpoint()
    if best:
        print(f"\næœ€ä½³æ£€æŸ¥ç‚¹: æ­¥éª¤ {best['step']}, æŸå¤± {best['loss']:.4f}")
```

### å¿«é€Ÿè¯„ä¼°

```bash
# è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹
python main.py --config config.yaml --mode eval --algorithm ppo

# è¯„ä¼°DPOæ¨¡å‹
python main.py --config config.yaml --mode eval --algorithm dpo

# è¯„ä¼°GRPOæ¨¡å‹
python main.py --config config.yaml --mode eval --algorithm grpo

# è¯„ä¼°ç‰¹å®šæ£€æŸ¥ç‚¹
python main.py --config config.yaml --mode eval --resume_from_checkpoint ./output/ppo/checkpoint-1000

# è¯„ä¼°å¹¶ç”ŸæˆæŠ¥å‘Š
python main.py --config config.yaml --mode eval --algorithm dpo --output_dir ./eval_results
```

### è‡ªåŠ¨è¯„ä¼°

```python
from src.evaluators import ModelEvaluator, AutomaticEvaluator
from transformers import AutoTokenizer

# åˆ›å»ºè¯„ä¼°å™¨
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
evaluator = ModelEvaluator(tokenizer=tokenizer)

# åŠ è½½æ¨¡å‹å’Œæ•°æ®
config = {
    'policy_model_path': './output/ppo/final_model',  # æˆ– './output/dpo/final_model', './output/grpo/final_model'
    'reward_model_path': './output/reward/final_model',
    'eval_dataset_path': './data/test.json',
    'batch_size': 8
}

# è¿è¡Œè¯„ä¼°
results = evaluator.evaluate(config)

# æŸ¥çœ‹ç»“æœ
print(f"BLEUåˆ†æ•°: {results['bleu_4']:.4f}")
print(f"ROUGE-Låˆ†æ•°: {results['rouge_l']:.4f}")
print(f"å¹³å‡å¥–åŠ±: {results['mean_reward']:.4f}")
print(f"å›°æƒ‘åº¦: {results['perplexity']:.2f}")
```

### ç”Ÿæˆè´¨é‡è¯„ä¼°

```python
from src.evaluators import AutomaticEvaluator
from src.models import create_policy_model

# åŠ è½½æ¨¡å‹
policy_model, tokenizer = create_policy_model("./output/ppo/final_model")  # æˆ–ä½¿ç”¨å…¶ä»–ç®—æ³•çš„æ¨¡å‹
reward_model, _ = create_reward_model("./output/reward/final_model")

# åˆ›å»ºè‡ªåŠ¨è¯„ä¼°å™¨
auto_evaluator = AutomaticEvaluator(
    tokenizer=tokenizer,
    output_dir="./eval_results"
)

# æµ‹è¯•æç¤º
test_prompts = [
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ",
    "æè¿°ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åº”ç”¨"
]

# å‚è€ƒç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
reference_answers = [
    "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯...",
    "æé«˜ç¼–ç¨‹æŠ€èƒ½éœ€è¦å¤šç»ƒä¹ ...",
    "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨..."
]

# ç»¼åˆè¯„ä¼°
results = auto_evaluator.evaluate_policy_model(
    policy_model=policy_model,
    reward_model=reward_model,
    prompts=test_prompts,
    references=reference_answers,
    generation_kwargs={
        'max_new_tokens': 128,
        'temperature': 0.7,
        'top_p': 0.9
    }
)

# ä¿å­˜ç»“æœ
result_file = auto_evaluator.save_results(results, "comprehensive_eval.json")
print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
```

### å¥–åŠ±æ¨¡å‹è¯„ä¼°

```python
from src.trainers import RewardModelTrainer
from src.evaluators import AutomaticEvaluator

# åŠ è½½å¥–åŠ±æ¨¡å‹
reward_trainer = RewardModelTrainer.from_pretrained("./output/reward/final_model")

# å‡†å¤‡åå¥½æ•°æ®
chosen_texts = ["è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å›ç­”", "è¯¦ç»†ä¸”å‡†ç¡®çš„è§£é‡Š"]
rejected_texts = ["å›ç­”ä¸å¤Ÿè¯¦ç»†", "ä¿¡æ¯æœ‰è¯¯"]

# è¯„ä¼°å¥–åŠ±æ¨¡å‹
auto_evaluator = AutomaticEvaluator(tokenizer=reward_trainer.tokenizer)
reward_metrics = auto_evaluator.evaluate_reward_model(
    reward_model=reward_trainer.model,
    chosen_texts=chosen_texts,
    rejected_texts=rejected_texts
)

print(f"åå¥½å‡†ç¡®ç‡: {reward_metrics['accuracy']:.4f}")
print(f"å¥–åŠ±å·®å¼‚: {reward_metrics['margin']:.4f}")
print(f"é€‰æ‹©å›ç­”å¹³å‡å¥–åŠ±: {reward_metrics['chosen_reward_mean']:.4f}")
print(f"æ‹’ç»å›ç­”å¹³å‡å¥–åŠ±: {reward_metrics['rejected_reward_mean']:.4f}")
```

### äººå·¥è¯„ä¼°

```python
from src.evaluators import HumanEvaluator

# åˆ›å»ºäººå·¥è¯„ä¼°å™¨
human_evaluator = HumanEvaluator(output_dir="./human_eval")

# å‡†å¤‡å¯¹æ¯”æ•°æ®
prompts = ["è§£é‡Šé‡å­è®¡ç®—çš„åŸç†", "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ"]
model_a_responses = ["é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦åŸç†...", "å­¦ä¹ Pythonå¯ä»¥ä»åŸºç¡€è¯­æ³•å¼€å§‹..."]
model_b_responses = ["é‡å­è®¡ç®—æ˜¯ä¸€ç§æ–°å‹è®¡ç®—æ–¹å¼...", "Pythonæ˜¯ä¸€é—¨æ˜“å­¦çš„ç¼–ç¨‹è¯­è¨€..."]

# åˆ›å»ºå¯¹æ¯”è¯„ä¼°ä»»åŠ¡
task_file = human_evaluator.create_evaluation_task(
    prompts=prompts,
    responses_a=model_a_responses,
    responses_b=model_b_responses,
    model_a_name="PPOæ¨¡å‹",
    model_b_name="DPOæ¨¡å‹",
    task_name="ppo_vs_dpo_comparison"
)

# å¯¼å‡ºHTMLè¯„ä¼°ç•Œé¢
html_file = human_evaluator.export_evaluation_interface(task_file, "html")
print(f"äººå·¥è¯„ä¼°ç•Œé¢: {html_file}")

# åˆ†æè¯„ä¼°ç»“æœï¼ˆè¯„ä¼°å®Œæˆåï¼‰
# results = human_evaluator.analyze_evaluation_results(task_file)
# print(f"PPOèƒœç‡: {results['win_rates']['PPOæ¨¡å‹']:.2%}")
```

### æ‰¹é‡æµ‹è¯•è„šæœ¬

```python
# test_models.py
import json
import argparse
from pathlib import Path
from src.evaluators import ModelEvaluator
from transformers import AutoTokenizer

def run_evaluation(model_path, test_data_path, output_dir):
    """è¿è¡Œæ¨¡å‹è¯„ä¼°"""
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
    evaluator = ModelEvaluator(tokenizer=tokenizer)
    
    # é…ç½®è¯„ä¼°
    config = {
        'policy_model_path': model_path,
        'eval_dataset_path': test_data_path,
        'batch_size': 8,
        'max_samples': len(test_data)
    }
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.evaluate(config)
    
    # ä¿å­˜ç»“æœ
    output_path = Path(output_dir) / f"eval_results_{Path(model_path).name}.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_path}")
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_data", required=True, help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--output_dir", default="./eval_results", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    results = run_evaluation(args.model_path, args.test_data, args.output_dir)
    
    # æ‰“å°å…³é”®æŒ‡æ ‡
    print("\n=== è¯„ä¼°ç»“æœ ===")
    print(f"BLEU-4: {results.get('bleu_4', 0):.4f}")
    print(f"ROUGE-L: {results.get('rouge_l', 0):.4f}")
    print(f"å¹³å‡å¥–åŠ±: {results.get('mean_reward', 0):.4f}")
    print(f"ç”Ÿæˆå¤šæ ·æ€§: {results.get('distinct_2', 0):.4f}")
```

### ä½¿ç”¨æ‰¹é‡æµ‹è¯•

```bash
# æµ‹è¯•PPOæ¨¡å‹
python test_models.py --model_path ./output/ppo/final_model --test_data ./data/test.json

# æµ‹è¯•DPOæ¨¡å‹
python test_models.py --model_path ./output/dpo/final_model --test_data ./data/test.json

# å¯¹æ¯”å¤šä¸ªæ¨¡å‹
for model in ./output/*/final_model; do
    echo "Testing $model"
    python test_models.py --model_path "$model" --test_data ./data/test.json
done
```

## ğŸ“ˆ ç›‘æ§å’Œå¯è§†åŒ–

### TensorBoardç›‘æ§

```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir ./output/logs

# åœ¨æµè§ˆå™¨ä¸­è®¿é—®
# http://localhost:6006
```

### è®­ç»ƒæŒ‡æ ‡

æ¡†æ¶è‡ªåŠ¨è®°å½•ä»¥ä¸‹æŒ‡æ ‡ï¼š

- **æŸå¤±æŒ‡æ ‡**: æ€»æŸå¤±ã€ç­–ç•¥æŸå¤±ã€ä»·å€¼æŸå¤±ã€ç†µæŸå¤±
- **PPOæŒ‡æ ‡**: è£å‰ªæ¯”ä¾‹ã€KLæ•£åº¦ã€ä¼˜åŠ¿ä¼°è®¡
- **DPOæŒ‡æ ‡**: åå¥½å‡†ç¡®ç‡ã€å¥–åŠ±å·®å¼‚ã€éšå¼å¥–åŠ±
- **ç”ŸæˆæŒ‡æ ‡**: å“åº”é•¿åº¦ã€ç”Ÿæˆè´¨é‡
- **è®­ç»ƒæŒ‡æ ‡**: å­¦ä¹ ç‡ã€æ¢¯åº¦èŒƒæ•°ã€è®­ç»ƒé€Ÿåº¦

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ç®—æ³•

```python
from src.algorithms import BaseRLAlgorithm

class CustomAlgorithm(BaseRLAlgorithm):
    def __init__(self, model, config):
        super().__init__(model, config)
        # è‡ªå®šä¹‰åˆå§‹åŒ–
    
    def compute_loss(self, batch):
        # å®ç°è‡ªå®šä¹‰æŸå¤±è®¡ç®—
        pass
    
    def training_step(self, batch):
        # å®ç°è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤
        pass
```

### è‡ªå®šä¹‰æ•°æ®é›†

```python
from src.data import RLDataset

class CustomDataset(RLDataset):
    def __init__(self, data_path, tokenizer, max_length):
        super().__init__(tokenizer, max_length)
        # åŠ è½½è‡ªå®šä¹‰æ•°æ®
    
    def __getitem__(self, idx):
        # å®ç°æ•°æ®è·å–é€»è¾‘
        pass
```

### è‡ªå®šä¹‰è®­ç»ƒå™¨

```python
from src.trainers import BaseTrainer

class CustomTrainer(BaseTrainer):
    def compute_loss(self, batch):
        # å®ç°è‡ªå®šä¹‰æŸå¤±è®¡ç®—
        pass
    
    def evaluate(self):
        # å®ç°è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
        pass
```

## ğŸ› æ•…éšœæ’é™¤

### ğŸš¨ æ™ºèƒ½é”™è¯¯å¤„ç†å’Œè°ƒè¯•

```python
# scripts/error_handler.py
import traceback
import logging
import sys
from datetime import datetime
from pathlib import Path

class TrainingErrorHandler:
    def __init__(self, log_dir="./logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
    def setup_logging(self):
        """è®¾ç½®è¯¦ç»†æ—¥å¿—è®°å½•"""
        log_file = self.log_dir / f"training_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        
    def handle_cuda_error(self, error):
        """å¤„ç†CUDAç›¸å…³é”™è¯¯"""
        error_msg = str(error).lower()
        
        if "out of memory" in error_msg:
            suggestions = [
                "ğŸ”§ å‡å°‘æ‰¹æ¬¡å¤§å° (batch_size)",
                "ğŸ”§ å¯ç”¨æ¢¯åº¦ç´¯ç§¯ (gradient_accumulation_steps)",
                "ğŸ”§ ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (fp16=True)",
                "ğŸ”§ å‡å°‘åºåˆ—é•¿åº¦ (max_length)",
                "ğŸ”§ ä½¿ç”¨CPUå¸è½½ (offload_to_cpu=True)"
            ]
            
            self.logger.error(f"âŒ CUDAå†…å­˜ä¸è¶³: {error}")
            self.logger.info("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            for suggestion in suggestions:
                self.logger.info(f"   {suggestion}")
                
            # è‡ªåŠ¨ç”Ÿæˆä¿®å¤é…ç½®
            self.generate_memory_optimized_config()
            
        elif "device-side assert" in error_msg:
            self.logger.error("âŒ CUDAè®¾å¤‡æ–­è¨€é”™è¯¯ï¼Œå¯èƒ½æ˜¯æ•°æ®é—®é¢˜")
            self.logger.info("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
            self.logger.info("   ğŸ” æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®")
            self.logger.info("   ğŸ” æ ‡ç­¾èŒƒå›´æ˜¯å¦åˆç†")
            self.logger.info("   ğŸ” è¾“å…¥é•¿åº¦æ˜¯å¦è¶…é™")
            
    def generate_memory_optimized_config(self):
        """ç”Ÿæˆå†…å­˜ä¼˜åŒ–é…ç½®"""
        optimized_config = {
            "training": {
                "batch_size": 1,
                "gradient_accumulation_steps": 16,
                "fp16": True,
                "dataloader_num_workers": 0,
                "save_steps": 100,
                "eval_steps": 100
            },
            "model": {
                "gradient_checkpointing": True
            },
            "data": {
                "max_length": 512
            }
        }
        
        config_file = self.log_dir / "memory_optimized_config.yaml"
        import yaml
        with open(config_file, 'w') as f:
            yaml.dump(optimized_config, f, default_flow_style=False)
            
        self.logger.info(f"âœ… å·²ç”Ÿæˆå†…å­˜ä¼˜åŒ–é…ç½®: {config_file}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    handler = TrainingErrorHandler()
    
    # åœ¨è®­ç»ƒä»£ç ä¸­ä½¿ç”¨
    try:
        # è®­ç»ƒä»£ç 
        pass
    except RuntimeError as e:
        if "cuda" in str(e).lower():
            handler.handle_cuda_error(e)
        raise
```

### âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# scripts/performance_optimizer.py
import torch
import psutil

class PerformanceOptimizer:
    def __init__(self):
        self.gpu_memory = self.get_gpu_memory()
        self.cpu_count = psutil.cpu_count()
        self.system_memory = psutil.virtual_memory().total / 1024**3
        
    def get_gpu_memory(self):
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            return torch.cuda.get_device_properties(0).total_memory / 1024**3
        return 0
        
    def optimize_training_args(self, base_config):
        """æ ¹æ®ç¡¬ä»¶é…ç½®ä¼˜åŒ–è®­ç»ƒå‚æ•°"""
        optimized = base_config.copy()
        
        # æ ¹æ®GPUå†…å­˜è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if self.gpu_memory > 0:
            if self.gpu_memory >= 24:  # 24GB+
                optimized['per_device_train_batch_size'] = 8
                optimized['gradient_accumulation_steps'] = 2
            elif self.gpu_memory >= 16:  # 16-24GB
                optimized['per_device_train_batch_size'] = 4
                optimized['gradient_accumulation_steps'] = 4
            elif self.gpu_memory >= 8:   # 8-16GB
                optimized['per_device_train_batch_size'] = 2
                optimized['gradient_accumulation_steps'] = 8
            else:  # <8GB
                optimized['per_device_train_batch_size'] = 1
                optimized['gradient_accumulation_steps'] = 16
                optimized['fp16'] = True
                optimized['gradient_checkpointing'] = True
        
        return optimized
        
    def print_optimization_report(self):
        """æ‰“å°ä¼˜åŒ–æŠ¥å‘Š"""
        print("=== æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š ===")
        print(f"ğŸ–¥ï¸  GPUå†…å­˜: {self.gpu_memory:.1f}GB")
        print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜: {self.system_memory:.1f}GB")
        print(f"âš™ï¸  CPUæ ¸å¿ƒ: {self.cpu_count}")
        
        if self.gpu_memory >= 24:
            print(f"ğŸ“Š å»ºè®®æ¨¡å‹: 7Bæˆ–æ›´å¤§æ¨¡å‹")
        elif self.gpu_memory >= 16:
            print(f"ğŸ“Š å»ºè®®æ¨¡å‹: 3B-7Bæ¨¡å‹")
        elif self.gpu_memory >= 8:
            print(f"ğŸ“Š å»ºè®®æ¨¡å‹: 1B-3Bæ¨¡å‹")
        else:
            print(f"ğŸ“Š å»ºè®®æ¨¡å‹: å°äº1Bæ¨¡å‹æˆ–ä½¿ç”¨CPU")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    optimizer = PerformanceOptimizer()
    optimizer.print_optimization_report()
```

```bash
# è¿è¡Œæ€§èƒ½ä¼˜åŒ–åˆ†æ
python scripts/performance_optimizer.py

# ä½¿ç”¨ä¼˜åŒ–åçš„é…ç½®è®­ç»ƒ
python scripts/error_handler.py --generate_config
python main.py --config logs/memory_optimized_config.yaml
```

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   - å‡å°‘æ‰¹å¤§å° (`per_device_train_batch_size`)
   - å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (`gradient_accumulation_steps`)
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (`fp16: true`)
   - ä½¿ç”¨è‡ªåŠ¨ä¼˜åŒ–: `python scripts/performance_optimizer.py`

2. **è®­ç»ƒä¸æ”¶æ•›**
   - è°ƒæ•´å­¦ä¹ ç‡
   - æ£€æŸ¥æ•°æ®è´¨é‡
   - è°ƒæ•´ç®—æ³•è¶…å‚æ•°
   - ä½¿ç”¨é”™è¯¯å¤„ç†å™¨: `python scripts/error_handler.py`

3. **ç”Ÿæˆè´¨é‡å·®**
   - å¢åŠ è®­ç»ƒæ•°æ®
   - è°ƒæ•´ç”Ÿæˆå‚æ•°
   - ä½¿ç”¨æ›´å¥½çš„åŸºç¡€æ¨¡å‹

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è°ƒè¯•æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•
config.num_epochs = 1
config.max_train_samples = 100
config.max_eval_samples = 50
```

## ğŸ“š APIæ–‡æ¡£

### æ ¸å¿ƒç±»

- `BaseTrainer`: åŸºç¡€è®­ç»ƒå™¨ç±»
- `PPOTrainer`: PPOç®—æ³•è®­ç»ƒå™¨
- `DPOTrainer`: DPOç®—æ³•è®­ç»ƒå™¨
- `PolicyModel`: ç­–ç•¥æ¨¡å‹
- `ValueModel`: ä»·å€¼æ¨¡å‹
- `RewardModel`: å¥–åŠ±æ¨¡å‹

### å·¥å…·å‡½æ•°

- `setup_logger()`: è®¾ç½®æ—¥å¿—è®°å½•
- `load_config()`: åŠ è½½é…ç½®æ–‡ä»¶
- `compute_advantages()`: è®¡ç®—ä¼˜åŠ¿å‡½æ•°
- `save_checkpoint()`: ä¿å­˜æ£€æŸ¥ç‚¹

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [Transformers](https://github.com/huggingface/transformers) - é¢„è®­ç»ƒæ¨¡å‹åº“
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [TensorBoard](https://www.tensorflow.org/tensorboard) - å¯è§†åŒ–å·¥å…·

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ Issue
- å‘é€é‚®ä»¶
- åŠ å…¥è®¨è®ºç¾¤

---

**Happy Training! ğŸš€**
