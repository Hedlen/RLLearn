#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå‰ç½®æ£€æŸ¥è„šæœ¬
"""

import torch
import psutil
import os
import sys
import yaml
import argparse
from pathlib import Path
from transformers import AutoTokenizer, AutoConfig

def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    print("=== ç³»ç»Ÿèµ„æºæ£€æŸ¥ ===")
    
    # GPUæ£€æŸ¥
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        
        total_gpu_memory = 0
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            total_gpu_memory += gpu_memory
            
            # æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
            torch.cuda.set_device(i)
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            print(f"      å·²åˆ†é…: {allocated:.1f}GB, å·²ç¼“å­˜: {cached:.1f}GB")
            
            if allocated / gpu_memory > 0.8:
                print(f"      âš ï¸  GPU {i} ä½¿ç”¨ç‡è¾ƒé«˜ ({allocated/gpu_memory*100:.1f}%)")
        
        print(f"æ€»GPUå†…å­˜: {total_gpu_memory:.1f}GB")
        
        # å»ºè®®æ¨¡å‹å¤§å°
        if total_gpu_memory >= 24:
            print("ğŸ“Š å»ºè®®æ¨¡å‹: 7Bæˆ–æ›´å¤§æ¨¡å‹")
        elif total_gpu_memory >= 16:
            print("ğŸ“Š å»ºè®®æ¨¡å‹: 3B-7Bæ¨¡å‹")
        elif total_gpu_memory >= 8:
            print("ğŸ“Š å»ºè®®æ¨¡å‹: 1B-3Bæ¨¡å‹")
        else:
            print("ğŸ“Š å»ºè®®æ¨¡å‹: å°äº1Bæ¨¡å‹")
            
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        print("   å»ºè®®å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æ›´å¥½æ€§èƒ½")
    
    # å†…å­˜æ£€æŸ¥
    memory = psutil.virtual_memory()
    print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜: {memory.total / 1024**3:.1f}GB (å¯ç”¨: {memory.available / 1024**3:.1f}GB)")
    
    if memory.available / 1024**3 < 4:
        print("âš ï¸  å¯ç”¨å†…å­˜ä¸è¶³ï¼Œå¯èƒ½å½±å“è®­ç»ƒ")
        return False
    
    # ç£ç›˜ç©ºé—´æ£€æŸ¥
    disk = psutil.disk_usage('.')
    print(f"ğŸ’¿ ç£ç›˜ç©ºé—´: {disk.free / 1024**3:.1f}GB å¯ç”¨")
    
    if disk.free / 1024**3 < 10:
        print("âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå»ºè®®è‡³å°‘ä¿ç•™10GBç©ºé—´")
        return False
    
    return True

def check_model_accessibility(model_name):
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯è®¿é—®"""
    print(f"\n=== æ¨¡å‹è®¿é—®æ£€æŸ¥: {model_name} ===")
    
    try:
        # æ£€æŸ¥åˆ†è¯å™¨
        print("æ£€æŸ¥åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        print(f"   ç‰¹æ®Štoken: pad={tokenizer.pad_token}, eos={tokenizer.eos_token}")
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        print("æ£€æŸ¥æ¨¡å‹é…ç½®...")
        config = AutoConfig.from_pretrained(model_name)
        print(f"âœ… æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹ç±»å‹: {config.model_type}")
        print(f"   éšè—å±‚å¤§å°: {config.hidden_size}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")
        print(f"   å±‚æ•°: {config.num_hidden_layers}")
        
        # ä¼°ç®—æ¨¡å‹å¤§å°
        if hasattr(config, 'num_parameters'):
            params = config.num_parameters
        else:
            # ç²—ç•¥ä¼°ç®—
            params = config.hidden_size * config.num_hidden_layers * 12  # ç®€åŒ–ä¼°ç®—
        
        params_b = params / 1e9
        print(f"   ä¼°ç®—å‚æ•°é‡: {params_b:.1f}B")
        
        # ä¼°ç®—å†…å­˜éœ€æ±‚
        memory_gb = params_b * 4 * 1.2  # 4å­—èŠ‚/å‚æ•° + 20%å¼€é”€
        print(f"   ä¼°ç®—å†…å­˜éœ€æ±‚: {memory_gb:.1f}GB (ä»…æ¨ç†)")
        print(f"   è®­ç»ƒå†…å­˜éœ€æ±‚: {memory_gb * 3:.1f}GB (åŒ…å«æ¢¯åº¦å’Œä¼˜åŒ–å™¨)")
        
        return True
        
    except Exception as e:
        error_msg = str(e).lower()
        if "connection" in error_msg or "timeout" in error_msg:
            print(f"âŒ ç½‘ç»œè¿æ¥é—®é¢˜: {e}")
            print("   å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. ä½¿ç”¨é•œåƒæº: export HF_ENDPOINT=https://hf-mirror.com")
            print("   3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
        else:
            print(f"âŒ æ¨¡å‹è®¿é—®å¤±è´¥: {e}")
        return False

def check_data_files(config_path="config.yaml"):
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    print(f"\n=== æ•°æ®æ–‡ä»¶æ£€æŸ¥ ===")
    
    try:
        if not os.path.exists(config_path):
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return True  # å¯èƒ½ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if 'data' not in config:
            print("âš ï¸  é…ç½®ä¸­æœªæ‰¾åˆ°dataéƒ¨åˆ†")
            return True
        
        data_config = config['data']
        
        if 'datasets' not in data_config:
            print("âš ï¸  é…ç½®ä¸­æœªæ‰¾åˆ°datasets")
            return True
        
        datasets = data_config['datasets']
        if not isinstance(datasets, list):
            print("âŒ datasetsåº”è¯¥æ˜¯åˆ—è¡¨æ ¼å¼")
            return False
        
        total_size = 0
        valid_datasets = 0
        
        for i, dataset in enumerate(datasets):
            if 'path' not in dataset:
                print(f"âš ï¸  æ•°æ®é›† {i+1}: ç¼ºå°‘pathé…ç½®")
                continue
            
            path = dataset['path']
            if not os.path.exists(path):
                print(f"âŒ æ•°æ®é›† {i+1}: {path} ä¸å­˜åœ¨")
                return False
            
            size = os.path.getsize(path) / 1024**2
            total_size += size
            valid_datasets += 1
            
            # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
            try:
                import json
                with open(path, 'r', encoding='utf-8') as f:
                    if path.endswith('.json'):
                        data = json.load(f)
                        if isinstance(data, list) and len(data) > 0:
                            print(f"âœ… æ•°æ®é›† {i+1}: {path} ({size:.1f}MB, {len(data)} æ¡è®°å½•)")
                            
                            # æ£€æŸ¥æ•°æ®æ ¼å¼
                            sample = data[0]
                            if isinstance(sample, dict):
                                keys = list(sample.keys())
                                print(f"      æ•°æ®å­—æ®µ: {keys}")
                            else:
                                print(f"      âš ï¸  æ•°æ®æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
                        else:
                            print(f"âš ï¸  æ•°æ®é›† {i+1}: {path} æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
                    else:
                        print(f"âœ… æ•°æ®é›† {i+1}: {path} ({size:.1f}MB)")
            except Exception as e:
                print(f"âš ï¸  æ•°æ®é›† {i+1}: {path} è¯»å–å¤±è´¥ - {e}")
        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æœ‰æ•ˆæ•°æ®é›†: {valid_datasets}")
        print(f"   æ€»æ•°æ®å¤§å°: {total_size:.1f}MB")
        
        if valid_datasets == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_training_config(config_path="config.yaml"):
    """æ£€æŸ¥è®­ç»ƒé…ç½®"""
    print(f"\n=== è®­ç»ƒé…ç½®æ£€æŸ¥ ===")
    
    try:
        if not os.path.exists(config_path):
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return True
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        if 'training' not in config:
            print("âš ï¸  é…ç½®ä¸­æœªæ‰¾åˆ°trainingéƒ¨åˆ†")
            return True
        
        training_config = config['training']
        
        # æ£€æŸ¥è¾“å‡ºç›®å½•
        output_dir = training_config.get('output_dir', './output')
        output_path = Path(output_dir)
        if not output_path.exists():
            print(f"ğŸ“ å°†åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
            output_path.mkdir(parents=True, exist_ok=True)
        else:
            print(f"âœ… è¾“å‡ºç›®å½•å­˜åœ¨: {output_dir}")
        
        # æ£€æŸ¥å…³é”®å‚æ•°
        batch_size = training_config.get('per_device_train_batch_size', 4)
        grad_accum = training_config.get('gradient_accumulation_steps', 1)
        effective_batch = batch_size * grad_accum
        
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°:")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   æ¢¯åº¦ç´¯ç§¯: {grad_accum}")
        print(f"   æœ‰æ•ˆæ‰¹æ¬¡: {effective_batch}")
        print(f"   å­¦ä¹ ç‡: {training_config.get('learning_rate', 'N/A')}")
        print(f"   è®­ç»ƒè½®æ•°: {training_config.get('num_train_epochs', 'N/A')}")
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†å†…å­˜ä¼˜åŒ–
        fp16 = training_config.get('fp16', False)
        gradient_checkpointing = training_config.get('gradient_checkpointing', False)
        
        print(f"ğŸ”§ å†…å­˜ä¼˜åŒ–:")
        print(f"   æ··åˆç²¾åº¦(fp16): {fp16}")
        print(f"   æ¢¯åº¦æ£€æŸ¥ç‚¹: {gradient_checkpointing}")
        
        # æ ¹æ®GPUå†…å­˜ç»™å‡ºå»ºè®®
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if gpu_memory < 8 and not fp16:
                print("ğŸ’¡ å»ºè®®å¯ç”¨fp16ä»¥èŠ‚çœå†…å­˜")
            
            if gpu_memory < 12 and batch_size > 2:
                print(f"ğŸ’¡ å»ºè®®å‡å°‘æ‰¹æ¬¡å¤§å°åˆ°2æˆ–æ›´å°")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒé…ç½®æ£€æŸ¥å¤±è´¥: {e}")
        return False

def estimate_training_time(config_path="config.yaml", model_name=None):
    """ä¼°ç®—è®­ç»ƒæ—¶é—´"""
    print(f"\n=== è®­ç»ƒæ—¶é—´ä¼°ç®— ===")
    
    try:
        # åŸºç¡€å‚æ•°
        base_time_per_step = 1.0  # ç§’/æ­¥ (åŸºå‡†å€¼)
        
        # GPUåŠ é€Ÿå› å­
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if gpu_memory >= 24:
                gpu_factor = 0.3
            elif gpu_memory >= 16:
                gpu_factor = 0.5
            elif gpu_memory >= 8:
                gpu_factor = 0.8
            else:
                gpu_factor = 1.2
            
            gpu_factor /= gpu_count  # å¤šGPUåŠ é€Ÿ
        else:
            gpu_factor = 10.0  # CPUè®­ç»ƒå¾ˆæ…¢
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            training_config = config.get('training', {})
            data_config = config.get('data', {})
            
            epochs = training_config.get('num_train_epochs', 3)
            batch_size = training_config.get('per_device_train_batch_size', 4)
            max_steps = training_config.get('max_steps')
            
            # ä¼°ç®—æ•°æ®é‡
            total_samples = 0
            if 'datasets' in data_config:
                for dataset in data_config['datasets']:
                    if 'path' in dataset and os.path.exists(dataset['path']):
                        try:
                            import json
                            with open(dataset['path'], 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if isinstance(data, list):
                                    total_samples += len(data)
                        except:
                            total_samples += 1000  # é»˜è®¤ä¼°ç®—
            
            if total_samples == 0:
                total_samples = 10000  # é»˜è®¤ä¼°ç®—
            
            # è®¡ç®—æ­¥æ•°
            if max_steps:
                total_steps = max_steps
            else:
                steps_per_epoch = total_samples // batch_size
                total_steps = steps_per_epoch * epochs
            
            # ä¼°ç®—æ—¶é—´
            time_per_step = base_time_per_step * gpu_factor
            total_time_seconds = total_steps * time_per_step
            
            hours = total_time_seconds // 3600
            minutes = (total_time_seconds % 3600) // 60
            
            print(f"ğŸ“Š è®­ç»ƒä¼°ç®—:")
            print(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
            print(f"   æ€»æ­¥æ•°: {total_steps:,}")
            print(f"   æ¯æ­¥æ—¶é—´: {time_per_step:.1f}ç§’")
            print(f"   é¢„è®¡æ€»æ—¶é—´: {hours:.0f}å°æ—¶ {minutes:.0f}åˆ†é’Ÿ")
            
            if hours > 24:
                print("âš ï¸  è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®è€ƒè™‘:")
                print("   1. å‡å°‘è®­ç»ƒè½®æ•°")
                print("   2. å¢åŠ æ‰¹æ¬¡å¤§å°")
                print("   3. ä½¿ç”¨æ›´å¼ºçš„GPU")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ—¶é—´ä¼°ç®—å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="è®­ç»ƒå‰ç½®æ£€æŸ¥")
    parser.add_argument("model_name", nargs='?', default="Qwen/Qwen2.5-3B-Instruct", help="æ¨¡å‹åç§°")
    parser.add_argument("--config", default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--skip_model_check", action="store_true", help="è·³è¿‡æ¨¡å‹æ£€æŸ¥")
    
    args = parser.parse_args()
    
    print("ğŸ” å¼€å§‹è®­ç»ƒå‰ç½®æ£€æŸ¥...\n")
    
    checks = [
        lambda: check_system_resources(),
        lambda: check_data_files(args.config),
        lambda: check_training_config(args.config),
        lambda: estimate_training_time(args.config, args.model_name)
    ]
    
    if not args.skip_model_check:
        checks.insert(1, lambda: check_model_accessibility(args.model_name))
    
    results = []
    for check in checks:
        try:
            result = check()
            results.append(result)
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
            results.append(False)
    
    # æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ¯ è®­ç»ƒå‰ç½®æ£€æŸ¥æ€»ç»“")
    print("="*50)
    
    passed = sum(results)
    total = len(results)
    
    if passed == total:
        print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒï¼")
        print("ğŸš€ å»ºè®®è¿è¡Œå‘½ä»¤:")
        if args.config and os.path.exists(args.config):
            print(f"   python main.py --config {args.config}")
        else:
            print(f"   python example_training.py --model_name {args.model_name}")
        return True
    else:
        print(f"âš ï¸  {total - passed} é¡¹æ£€æŸ¥æœªé€šè¿‡")
        print("è¯·æ ¹æ®ä¸Šè¿°æç¤ºè§£å†³é—®é¢˜åé‡æ–°æ£€æŸ¥")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)