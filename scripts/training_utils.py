#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå·¥å…·ï¼šé”™è¯¯å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–
"""

import torch
import psutil
import gc
import os
import yaml
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import traceback
from datetime import datetime

class TrainingErrorHandler:
    """è®­ç»ƒé”™è¯¯å¤„ç†å™¨"""
    
    def __init__(self, output_dir: str = "./output"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.error_log = self.output_dir / "error_log.json"
        self.errors = []
    
    def handle_cuda_oom(self, error: Exception, config: Dict = None) -> Dict[str, Any]:
        """å¤„ç†CUDAå†…å­˜ä¸è¶³é”™è¯¯"""
        print("\nâŒ CUDAå†…å­˜ä¸è¶³é”™è¯¯")
        print(f"é”™è¯¯ä¿¡æ¯: {str(error)}")
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            
            # æ˜¾ç¤ºå½“å‰GPUä½¿ç”¨æƒ…å†µ
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"GPU {i}: {allocated:.1f}GB / {total:.1f}GB (ç¼“å­˜: {cached:.1f}GB)")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        suggestions = self._generate_memory_optimization(config)
        
        # è®°å½•é”™è¯¯
        error_info = {
            "timestamp": datetime.now().isoformat(),
            "error_type": "CUDA_OOM",
            "error_message": str(error),
            "suggestions": suggestions,
            "config": config
        }
        
        self.errors.append(error_info)
        self._save_error_log()
        
        print("\nğŸ’¡ å†…å­˜ä¼˜åŒ–å»ºè®®:")
        for suggestion in suggestions:
            print(f"   â€¢ {suggestion}")
        
        return suggestions
    
    def handle_device_assertion_error(self, error: Exception) -> Dict[str, Any]:
        """å¤„ç†è®¾å¤‡æ–­è¨€é”™è¯¯"""
        print("\nâŒ è®¾å¤‡æ–­è¨€é”™è¯¯")
        print(f"é”™è¯¯ä¿¡æ¯: {str(error)}")
        
        suggestions = [
            "æ£€æŸ¥æ‰€æœ‰å¼ é‡æ˜¯å¦åœ¨åŒä¸€è®¾å¤‡ä¸Š",
            "ç¡®ä¿æ¨¡å‹å’Œæ•°æ®åœ¨ç›¸åŒçš„GPUä¸Š",
            "æ£€æŸ¥æ˜¯å¦æœ‰å¼ é‡ä»åœ¨CPUä¸Š",
            "ä½¿ç”¨ .to(device) å°†å¼ é‡ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡",
            "æ£€æŸ¥å¤šGPUè®¾ç½®æ˜¯å¦æ­£ç¡®"
        ]
        
        # æ˜¾ç¤ºå½“å‰è®¾å¤‡ä¿¡æ¯
        if torch.cuda.is_available():
            print(f"\nğŸ” å½“å‰è®¾å¤‡ä¿¡æ¯:")
            print(f"   CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
            print(f"   å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
            for i in range(torch.cuda.device_count()):
                print(f"   è®¾å¤‡ {i}: {torch.cuda.get_device_name(i)}")
        
        error_info = {
            "timestamp": datetime.now().isoformat(),
            "error_type": "DEVICE_ASSERTION",
            "error_message": str(error),
            "suggestions": suggestions
        }
        
        self.errors.append(error_info)
        self._save_error_log()
        
        print("\nğŸ’¡ è§£å†³å»ºè®®:")
        for suggestion in suggestions:
            print(f"   â€¢ {suggestion}")
        
        return {"suggestions": suggestions}
    
    def handle_model_loading_error(self, error: Exception, model_name: str) -> Dict[str, Any]:
        """å¤„ç†æ¨¡å‹åŠ è½½é”™è¯¯"""
        print(f"\nâŒ æ¨¡å‹åŠ è½½é”™è¯¯: {model_name}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(error)}")
        
        error_msg = str(error).lower()
        suggestions = []
        
        if "connection" in error_msg or "timeout" in error_msg:
            suggestions.extend([
                "æ£€æŸ¥ç½‘ç»œè¿æ¥",
                "è®¾ç½®HuggingFaceé•œåƒ: export HF_ENDPOINT=https://hf-mirror.com",
                "ä½¿ç”¨ä»£ç†æˆ–VPN",
                "æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°",
                "å¢åŠ è¶…æ—¶æ—¶é—´"
            ])
        elif "not found" in error_msg or "does not exist" in error_msg:
            suggestions.extend([
                "æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤æ¨¡å‹åœ¨HuggingFace Hubä¸Šå­˜åœ¨",
                "æ£€æŸ¥æœ¬åœ°è·¯å¾„æ˜¯å¦æ­£ç¡®",
                "å°è¯•ä½¿ç”¨å®Œæ•´çš„æ¨¡å‹è·¯å¾„"
            ])
        elif "permission" in error_msg or "access" in error_msg:
            suggestions.extend([
                "æ£€æŸ¥æ–‡ä»¶æƒé™",
                "ç¡®è®¤æœ‰è®¿é—®æ¨¡å‹çš„æƒé™",
                "ç™»å½•HuggingFace: huggingface-cli login",
                "æ£€æŸ¥ç§æœ‰æ¨¡å‹çš„è®¿é—®æƒé™"
            ])
        else:
            suggestions.extend([
                "æ£€æŸ¥æ¨¡å‹æ ¼å¼æ˜¯å¦å…¼å®¹",
                "æ›´æ–°transformersåº“ç‰ˆæœ¬",
                "å°è¯•ä½¿ç”¨ä¸åŒçš„æ¨¡å‹ç‰ˆæœ¬",
                "æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³"
            ])
        
        error_info = {
            "timestamp": datetime.now().isoformat(),
            "error_type": "MODEL_LOADING",
            "model_name": model_name,
            "error_message": str(error),
            "suggestions": suggestions
        }
        
        self.errors.append(error_info)
        self._save_error_log()
        
        print("\nğŸ’¡ è§£å†³å»ºè®®:")
        for suggestion in suggestions:
            print(f"   â€¢ {suggestion}")
        
        return {"suggestions": suggestions}
    
    def _generate_memory_optimization(self, config: Dict = None) -> List[str]:
        """ç”Ÿæˆå†…å­˜ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        if config:
            batch_size = config.get('training', {}).get('per_device_train_batch_size', 4)
            if batch_size > 2:
                suggestions.append(f"å‡å°‘æ‰¹æ¬¡å¤§å°: å½“å‰ {batch_size} -> å»ºè®® {max(1, batch_size // 2)}")
            
            if not config.get('training', {}).get('fp16', False):
                suggestions.append("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ: fp16=True")
            
            if not config.get('training', {}).get('gradient_checkpointing', False):
                suggestions.append("å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: gradient_checkpointing=True")
            
            grad_accum = config.get('training', {}).get('gradient_accumulation_steps', 1)
            if grad_accum < 4:
                suggestions.append(f"å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: å½“å‰ {grad_accum} -> å»ºè®® {grad_accum * 2}")
        
        # é€šç”¨å»ºè®®
        suggestions.extend([
            "ä½¿ç”¨æ›´å°çš„æ¨¡å‹",
            "å‡å°‘åºåˆ—é•¿åº¦",
            "å¯ç”¨CPUå¸è½½",
            "ä½¿ç”¨DeepSpeed ZeRO",
            "æ¸…ç†ä¸å¿…è¦çš„å˜é‡"
        ])
        
        return suggestions
    
    def generate_optimized_config(self, original_config: Dict, error_type: str = "CUDA_OOM") -> Dict:
        """ç”Ÿæˆä¼˜åŒ–åçš„é…ç½®"""
        optimized_config = original_config.copy()
        
        if error_type == "CUDA_OOM":
            training_config = optimized_config.setdefault('training', {})
            
            # å‡å°‘æ‰¹æ¬¡å¤§å°
            current_batch = training_config.get('per_device_train_batch_size', 4)
            training_config['per_device_train_batch_size'] = max(1, current_batch // 2)
            
            # å¯ç”¨å†…å­˜ä¼˜åŒ–
            training_config['fp16'] = True
            training_config['gradient_checkpointing'] = True
            training_config['dataloader_pin_memory'] = False
            
            # å¢åŠ æ¢¯åº¦ç´¯ç§¯
            current_accum = training_config.get('gradient_accumulation_steps', 1)
            training_config['gradient_accumulation_steps'] = current_accum * 2
            
            # å‡å°‘åºåˆ—é•¿åº¦
            if 'max_length' in training_config:
                current_length = training_config['max_length']
                training_config['max_length'] = min(current_length, 1024)
        
        # ä¿å­˜ä¼˜åŒ–é…ç½®
        optimized_path = self.output_dir / "optimized_config.yaml"
        with open(optimized_path, 'w', encoding='utf-8') as f:
            yaml.dump(optimized_config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"\nğŸ’¾ ä¼˜åŒ–é…ç½®å·²ä¿å­˜: {optimized_path}")
        return optimized_config
    
    def _save_error_log(self):
        """ä¿å­˜é”™è¯¯æ—¥å¿—"""
        try:
            with open(self.error_log, 'w', encoding='utf-8') as f:
                json.dump(self.errors, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜é”™è¯¯æ—¥å¿—å¤±è´¥: {e}")
    
    def print_error_summary(self):
        """æ‰“å°é”™è¯¯æ‘˜è¦"""
        if not self.errors:
            print("âœ… æ²¡æœ‰è®°å½•çš„é”™è¯¯")
            return
        
        print(f"\nğŸ“‹ é”™è¯¯æ‘˜è¦ (å…± {len(self.errors)} ä¸ªé”™è¯¯):")
        
        error_types = {}
        for error in self.errors:
            error_type = error['error_type']
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        for error_type, count in error_types.items():
            print(f"   {error_type}: {count} æ¬¡")
        
        print(f"\næœ€è¿‘çš„é”™è¯¯:")
        for error in self.errors[-3:]:
            print(f"   [{error['timestamp']}] {error['error_type']}: {error['error_message'][:100]}...")

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.system_info = self._get_system_info()
    
    def _get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            "cpu_count": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / 1024**3,
            "gpu_count": 0,
            "gpu_memory_gb": 0,
            "gpu_names": []
        }
        
        if torch.cuda.is_available():
            info["gpu_count"] = torch.cuda.device_count()
            total_gpu_memory = 0
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                
                info["gpu_names"].append(gpu_name)
                total_gpu_memory += gpu_memory
            
            info["gpu_memory_gb"] = total_gpu_memory
        
        return info
    
    def optimize_training_config(self, config: Dict = None) -> Dict:
        """ä¼˜åŒ–è®­ç»ƒé…ç½®"""
        if config is None:
            config = {}
        
        optimized = config.copy()
        training_config = optimized.setdefault('training', {})
        
        # æ ¹æ®GPUå†…å­˜ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
        if self.system_info["gpu_count"] > 0:
            gpu_memory = self.system_info["gpu_memory_gb"]
            
            if gpu_memory >= 24:
                # é«˜ç«¯GPU
                training_config.setdefault('per_device_train_batch_size', 8)
                training_config.setdefault('gradient_accumulation_steps', 2)
                training_config.setdefault('max_length', 2048)
            elif gpu_memory >= 16:
                # ä¸­ç«¯GPU
                training_config.setdefault('per_device_train_batch_size', 4)
                training_config.setdefault('gradient_accumulation_steps', 4)
                training_config.setdefault('max_length', 1536)
            elif gpu_memory >= 8:
                # å…¥é—¨GPU
                training_config.setdefault('per_device_train_batch_size', 2)
                training_config.setdefault('gradient_accumulation_steps', 8)
                training_config.setdefault('max_length', 1024)
                training_config['fp16'] = True
                training_config['gradient_checkpointing'] = True
            else:
                # ä½ç«¯GPU
                training_config.setdefault('per_device_train_batch_size', 1)
                training_config.setdefault('gradient_accumulation_steps', 16)
                training_config.setdefault('max_length', 512)
                training_config['fp16'] = True
                training_config['gradient_checkpointing'] = True
                training_config['dataloader_pin_memory'] = False
        else:
            # CPUè®­ç»ƒ
            training_config.setdefault('per_device_train_batch_size', 1)
            training_config.setdefault('gradient_accumulation_steps', 32)
            training_config.setdefault('max_length', 512)
            training_config['dataloader_num_workers'] = min(4, self.system_info["cpu_count"])
        
        # æ ¹æ®CPUæ ¸å¿ƒæ•°ä¼˜åŒ–æ•°æ®åŠ è½½
        cpu_count = self.system_info["cpu_count"]
        if cpu_count >= 16:
            training_config.setdefault('dataloader_num_workers', 8)
        elif cpu_count >= 8:
            training_config.setdefault('dataloader_num_workers', 4)
        else:
            training_config.setdefault('dataloader_num_workers', 2)
        
        # å†…å­˜ä¼˜åŒ–
        memory_gb = self.system_info["memory_gb"]
        if memory_gb < 16:
            training_config['dataloader_pin_memory'] = False
            training_config.setdefault('max_length', 512)
        
        return optimized
    
    def suggest_model_size(self) -> Dict[str, str]:
        """å»ºè®®æ¨¡å‹å¤§å°"""
        gpu_memory = self.system_info["gpu_memory_gb"]
        
        if gpu_memory >= 40:
            return {
                "recommended": "13B-30B",
                "models": ["Qwen2.5-14B", "Llama-2-13B", "Baichuan2-13B"],
                "reason": "é«˜ç«¯GPUï¼Œå¯ä»¥è®­ç»ƒå¤§å‹æ¨¡å‹"
            }
        elif gpu_memory >= 24:
            return {
                "recommended": "7B-13B",
                "models": ["Qwen2.5-7B", "Llama-2-7B", "ChatGLM3-6B"],
                "reason": "ä¸­é«˜ç«¯GPUï¼Œé€‚åˆä¸­å¤§å‹æ¨¡å‹"
            }
        elif gpu_memory >= 16:
            return {
                "recommended": "3B-7B",
                "models": ["Qwen2.5-3B", "Phi-3-mini", "ChatGLM3-6B"],
                "reason": "ä¸­ç«¯GPUï¼Œé€‚åˆä¸­å‹æ¨¡å‹"
            }
        elif gpu_memory >= 8:
            return {
                "recommended": "1B-3B",
                "models": ["Qwen2.5-1.5B", "Phi-3-mini", "TinyLlama-1.1B"],
                "reason": "å…¥é—¨GPUï¼Œå»ºè®®å°å‹æ¨¡å‹"
            }
        else:
            return {
                "recommended": "<1B",
                "models": ["TinyLlama-1.1B", "DistilBERT"],
                "reason": "GPUå†…å­˜ä¸è¶³ï¼Œåªèƒ½ä½¿ç”¨å¾ˆå°çš„æ¨¡å‹"
            }
    
    def print_optimization_report(self, config: Dict = None):
        """æ‰“å°ä¼˜åŒ–æŠ¥å‘Š"""
        print("\nğŸ”§ æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š")
        print("=" * 50)
        
        # ç³»ç»Ÿä¿¡æ¯
        print(f"ğŸ’» ç³»ç»Ÿé…ç½®:")
        print(f"   CPU: {self.system_info['cpu_count']} æ ¸å¿ƒ")
        print(f"   å†…å­˜: {self.system_info['memory_gb']:.1f}GB")
        
        if self.system_info["gpu_count"] > 0:
            print(f"   GPU: {self.system_info['gpu_count']} ä¸ª")
            print(f"   GPUå†…å­˜: {self.system_info['gpu_memory_gb']:.1f}GB")
            for i, name in enumerate(self.system_info["gpu_names"]):
                print(f"     GPU {i}: {name}")
        else:
            print(f"   GPU: æ— ")
        
        # æ¨¡å‹å»ºè®®
        model_suggestion = self.suggest_model_size()
        print(f"\nğŸ¤– æ¨èæ¨¡å‹å¤§å°: {model_suggestion['recommended']}")
        print(f"   åŸå› : {model_suggestion['reason']}")
        print(f"   æ¨èæ¨¡å‹:")
        for model in model_suggestion['models']:
            print(f"     â€¢ {model}")
        
        # é…ç½®ä¼˜åŒ–
        if config:
            optimized = self.optimize_training_config(config)
            training_config = optimized.get('training', {})
            
            print(f"\nâš™ï¸  ä¼˜åŒ–åçš„è®­ç»ƒé…ç½®:")
            print(f"   æ‰¹æ¬¡å¤§å°: {training_config.get('per_device_train_batch_size', 'N/A')}")
            print(f"   æ¢¯åº¦ç´¯ç§¯: {training_config.get('gradient_accumulation_steps', 'N/A')}")
            print(f"   æœ€å¤§é•¿åº¦: {training_config.get('max_length', 'N/A')}")
            print(f"   æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹: {training_config.get('dataloader_num_workers', 'N/A')}")
            print(f"   æ··åˆç²¾åº¦: {training_config.get('fp16', False)}")
            print(f"   æ¢¯åº¦æ£€æŸ¥ç‚¹: {training_config.get('gradient_checkpointing', False)}")
            
            # è®¡ç®—æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
            batch_size = training_config.get('per_device_train_batch_size', 1)
            grad_accum = training_config.get('gradient_accumulation_steps', 1)
            gpu_count = max(1, self.system_info["gpu_count"])
            effective_batch = batch_size * grad_accum * gpu_count
            
            print(f"   æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch}")
        
        # æ€§èƒ½æç¤º
        print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–æç¤º:")
        
        if self.system_info["gpu_count"] == 0:
            print(f"   â€¢ è€ƒè™‘ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ")
            print(f"   â€¢ CPUè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ä½¿ç”¨å°æ¨¡å‹")
        elif self.system_info["gpu_memory_gb"] < 8:
            print(f"   â€¢ GPUå†…å­˜è¾ƒå°ï¼Œå¯ç”¨fp16å’Œæ¢¯åº¦æ£€æŸ¥ç‚¹")
            print(f"   â€¢ è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°")
        
        if self.system_info["memory_gb"] < 16:
            print(f"   â€¢ ç³»ç»Ÿå†…å­˜è¾ƒå°ï¼Œå…³é—­pin_memory")
            print(f"   â€¢ å‡å°‘æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°")
        
        if self.system_info["cpu_count"] < 4:
            print(f"   â€¢ CPUæ ¸å¿ƒæ•°è¾ƒå°‘ï¼Œå¯èƒ½å½±å“æ•°æ®åŠ è½½é€Ÿåº¦")
        
        print(f"   â€¢ å®šæœŸæ¸…ç†GPUç¼“å­˜: torch.cuda.empty_cache()")
        print(f"   â€¢ ä½¿ç”¨TensorBoardç›‘æ§è®­ç»ƒè¿›åº¦")
        print(f"   â€¢ è€ƒè™‘ä½¿ç”¨DeepSpeedè¿›ä¸€æ­¥ä¼˜åŒ–")

def main():
    """æµ‹è¯•å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒå·¥å…·æµ‹è¯•")
    parser.add_argument("--action", choices=["optimize", "error_test"], default="optimize", help="æ“ä½œç±»å‹")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    if args.action == "optimize":
        optimizer = PerformanceOptimizer()
        
        config = None
        if args.config and os.path.exists(args.config):
            with open(args.config, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        
        optimizer.print_optimization_report(config)
        
        if config:
            optimized = optimizer.optimize_training_config(config)
            print(f"\nğŸ’¾ ä¼˜åŒ–åçš„é…ç½®:")
            print(yaml.dump(optimized, default_flow_style=False, allow_unicode=True))
    
    elif args.action == "error_test":
        handler = TrainingErrorHandler()
        
        # æ¨¡æ‹ŸCUDA OOMé”™è¯¯
        try:
            raise RuntimeError("CUDA out of memory. Tried to allocate 2.00 GiB")
        except Exception as e:
            handler.handle_cuda_oom(e, {"training": {"per_device_train_batch_size": 8}})
        
        handler.print_error_summary()

if __name__ == "__main__":
    main()