#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¯å¢ƒæ£€æŸ¥è„šæœ¬
"""

import sys
import os
import platform
import subprocess
import importlib
from pathlib import Path

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print("=== Pythonç¯å¢ƒæ£€æŸ¥ ===")
    
    version = sys.version_info
    version_str = f"{version.major}.{version.minor}.{version.micro}"
    print(f"Pythonç‰ˆæœ¬: {version_str}")
    
    if version < (3, 8):
        print("âŒ Pythonç‰ˆæœ¬è¿‡ä½ï¼Œå»ºè®®ä½¿ç”¨3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False
    elif version >= (3, 11):
        print("âœ… Pythonç‰ˆæœ¬è‰¯å¥½")
    else:
        print("âœ… Pythonç‰ˆæœ¬å¯ç”¨")
    
    print(f"Pythonè·¯å¾„: {sys.executable}")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    return True

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒ"""
    print("\n=== GPUç¯å¢ƒæ£€æŸ¥ ===")
    
    try:
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"âœ… CUDAå¯ç”¨ï¼Œæ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            # æµ‹è¯•GPUå¯ç”¨æ€§
            try:
                test_tensor = torch.randn(100, 100).cuda()
                print("âœ… GPUæµ‹è¯•é€šè¿‡")
                del test_tensor
                torch.cuda.empty_cache()
            except Exception as e:
                print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
                return False
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
            print("   å¦‚éœ€GPUåŠ é€Ÿï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
        
        return True
        
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ GPUæ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_required_packages():
    """æ£€æŸ¥å¿…éœ€çš„PythonåŒ…"""
    print("\n=== ä¾èµ–åŒ…æ£€æŸ¥ ===")
    
    required_packages = [
        ('torch', 'PyTorch', '>=1.12.0'),
        ('transformers', 'Transformers', '>=4.21.0'),
        ('datasets', 'Datasets', '>=2.0.0'),
        ('numpy', 'NumPy', '>=1.20.0'),
        ('pandas', 'Pandas', '>=1.3.0'),
        ('yaml', 'PyYAML', '>=5.4.0'),
        ('tqdm', 'tqdm', '>=4.60.0'),
        ('tensorboard', 'TensorBoard', '>=2.8.0'),
        ('accelerate', 'Accelerate', '>=0.12.0'),
        ('peft', 'PEFT', '>=0.3.0')
    ]
    
    missing_packages = []
    installed_packages = []
    
    for package_name, display_name, min_version in required_packages:
        try:
            module = importlib.import_module(package_name)
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {display_name}: {version}")
            installed_packages.append((package_name, version))
        except ImportError:
            print(f"âŒ {display_name}: æœªå®‰è£…")
            missing_packages.append(package_name)
    
    if missing_packages:
        print(f"\nç¼ºå¤±çš„åŒ…: {', '.join(missing_packages)}")
        print(f"å®‰è£…å‘½ä»¤: pip install {' '.join(missing_packages)}")
        return False
    
    print("\nâœ… æ‰€æœ‰å¿…éœ€åŒ…å·²å®‰è£…")
    return True

def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    print("\n=== ç³»ç»Ÿèµ„æºæ£€æŸ¥ ===")
    
    try:
        import psutil
        
        # å†…å­˜æ£€æŸ¥
        memory = psutil.virtual_memory()
        memory_gb = memory.total / 1024**3
        available_gb = memory.available / 1024**3
        
        print(f"ç³»ç»Ÿå†…å­˜: {memory_gb:.1f}GB (å¯ç”¨: {available_gb:.1f}GB)")
        
        if memory_gb < 8:
            print("âš ï¸  ç³»ç»Ÿå†…å­˜è¾ƒå°‘ï¼Œå»ºè®®è‡³å°‘8GB")
        elif memory_gb >= 16:
            print("âœ… ç³»ç»Ÿå†…å­˜å……è¶³")
        else:
            print("âœ… ç³»ç»Ÿå†…å­˜å¯ç”¨")
        
        # CPUæ£€æŸ¥
        cpu_count = psutil.cpu_count()
        cpu_count_logical = psutil.cpu_count(logical=True)
        print(f"CPUæ ¸å¿ƒ: {cpu_count} ç‰©ç†æ ¸å¿ƒ, {cpu_count_logical} é€»è¾‘æ ¸å¿ƒ")
        
        # ç£ç›˜ç©ºé—´æ£€æŸ¥
        disk = psutil.disk_usage('.')
        free_gb = disk.free / 1024**3
        total_gb = disk.total / 1024**3
        
        print(f"ç£ç›˜ç©ºé—´: {free_gb:.1f}GB å¯ç”¨ / {total_gb:.1f}GB æ€»è®¡")
        
        if free_gb < 10:
            print("âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå»ºè®®è‡³å°‘ä¿ç•™10GB")
        else:
            print("âœ… ç£ç›˜ç©ºé—´å……è¶³")
        
        return True
        
    except ImportError:
        print("âš ï¸  psutilæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥ç³»ç»Ÿèµ„æº")
        print("   å®‰è£…å‘½ä»¤: pip install psutil")
        return True  # ä¸æ˜¯å¿…éœ€çš„ï¼Œæ‰€ä»¥è¿”å›True
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_huggingface_access():
    """æ£€æŸ¥HuggingFaceè®¿é—®"""
    print("\n=== HuggingFaceè®¿é—®æ£€æŸ¥ ===")
    
    try:
        from transformers import AutoTokenizer
        
        # æµ‹è¯•æ¨¡å‹è®¿é—®
        test_model = "Qwen/Qwen2.5-3B-Instruct"
        print(f"æµ‹è¯•æ¨¡å‹è®¿é—®: {test_model}")
        
        try:
            tokenizer = AutoTokenizer.from_pretrained(test_model)
            print("âœ… HuggingFaceæ¨¡å‹è®¿é—®æ­£å¸¸")
            return True
        except Exception as e:
            error_msg = str(e).lower()
            if "connection" in error_msg or "timeout" in error_msg:
                print("âŒ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•è®¿é—®HuggingFace")
                print("   å»ºè®®:")
                print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                print("   2. ä½¿ç”¨é•œåƒæº: export HF_ENDPOINT=https://hf-mirror.com")
                print("   3. ä½¿ç”¨ä»£ç†æˆ–VPN")
            else:
                print(f"âŒ HuggingFaceè®¿é—®å¤±è´¥: {e}")
            return False
            
    except ImportError:
        print("âŒ transformersæœªå®‰è£…")
        return False

def check_project_structure():
    """æ£€æŸ¥é¡¹ç›®ç»“æ„"""
    print("\n=== é¡¹ç›®ç»“æ„æ£€æŸ¥ ===")
    
    required_dirs = ['src', 'data', 'output', 'logs']
    required_files = ['config.yaml', 'requirements.txt']
    
    # æ£€æŸ¥ç›®å½•
    for dir_name in required_dirs:
        dir_path = Path(dir_name)
        if dir_path.exists():
            print(f"âœ… ç›®å½•å­˜åœ¨: {dir_name}/")
        else:
            print(f"ğŸ“ åˆ›å»ºç›®å½•: {dir_name}/")
            dir_path.mkdir(exist_ok=True)
    
    # æ£€æŸ¥æ–‡ä»¶
    for file_name in required_files:
        file_path = Path(file_name)
        if file_path.exists():
            print(f"âœ… æ–‡ä»¶å­˜åœ¨: {file_name}")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_name}")
    
    return True

def generate_environment_report():
    """ç”Ÿæˆç¯å¢ƒæŠ¥å‘Š"""
    print("\n=== ç”Ÿæˆç¯å¢ƒæŠ¥å‘Š ===")
    
    report = {
        'python_version': f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
        'platform': f"{platform.system()} {platform.release()}",
        'python_path': sys.executable
    }
    
    # GPUä¿¡æ¯
    try:
        import torch
        report['pytorch_version'] = torch.__version__
        report['cuda_available'] = torch.cuda.is_available()
        if torch.cuda.is_available():
            report['gpu_count'] = torch.cuda.device_count()
            report['gpu_names'] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]
    except ImportError:
        report['pytorch_version'] = 'Not installed'
        report['cuda_available'] = False
    
    # ç³»ç»Ÿèµ„æº
    try:
        import psutil
        memory = psutil.virtual_memory()
        report['total_memory_gb'] = round(memory.total / 1024**3, 1)
        report['available_memory_gb'] = round(memory.available / 1024**3, 1)
        report['cpu_count'] = psutil.cpu_count()
        
        disk = psutil.disk_usage('.')
        report['free_disk_gb'] = round(disk.free / 1024**3, 1)
    except ImportError:
        pass
    
    # ä¿å­˜æŠ¥å‘Š
    import json
    report_file = Path('logs/environment_report.json')
    report_file.parent.mkdir(exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç¯å¢ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    return report_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹ç¯å¢ƒæ£€æŸ¥...\n")
    
    checks = [
        check_python_version,
        check_required_packages,
        check_gpu_environment,
        check_system_resources,
        check_huggingface_access,
        check_project_structure
    ]
    
    results = []
    for check in checks:
        try:
            result = check()
            results.append(result)
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
            results.append(False)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_environment_report()
    
    # æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ¯ ç¯å¢ƒæ£€æŸ¥æ€»ç»“")
    print("="*50)
    
    passed = sum(results)
    total = len(results)
    
    if passed == total:
        print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œç¯å¢ƒé…ç½®è‰¯å¥½ï¼")
        print("ğŸš€ å¯ä»¥å¼€å§‹è®­ç»ƒäº†")
        return True
    else:
        print(f"âš ï¸  {total - passed} é¡¹æ£€æŸ¥æœªé€šè¿‡")
        print("è¯·æ ¹æ®ä¸Šè¿°æç¤ºè§£å†³é—®é¢˜åé‡æ–°æ£€æŸ¥")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)